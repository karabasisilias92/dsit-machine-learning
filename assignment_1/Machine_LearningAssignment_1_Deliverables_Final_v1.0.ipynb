{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6kYAu1b6oL8"
   },
   "source": [
    "# Machine Learning - DSIT UOA\n",
    "\n",
    "*** Assignment 1 ***\n",
    "\n",
    "<u>**Contributors:**</u> Christina Borovilou (ds1200008), Ilias Stylianos Karampasis (ds1200005), Anastasia Rempoulaki (ds1200016)\n",
    "\n",
    "General comments regarding the below deliverables:\n",
    "\n",
    "- Each question can be run independently and all code regarding its solution has been grouped in a collapsible heading for better and easier observation. *In case of you are using Anaconda, extensions for jupyter notebooks must be installed and then enabling of collapsible heading if you want to use this functionalilty. See more here: https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html*\n",
    "- We distributed the work for each problem. So although everyone in the team has knowledge of the solution for every individual question, you are going to observe diffent coding styles and perhaps level of details in commenting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFfzGA3V6s6r"
   },
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eF3ETV56zNJ"
   },
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npHBwhZ--O6I"
   },
   "outputs": [],
   "source": [
    "# importing the required module \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S70eopB4-O6K"
   },
   "source": [
    "Function to generate our **Input** & **\"Experimental\"** Results in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWZFBet--O6K"
   },
   "outputs": [],
   "source": [
    "def y_actual_res(th0, th1, th2, th3, th5, r_step, r__start, r__end):\n",
    "\n",
    "    x = np.linspace(r_start, r_end, num= N)     #includes 2 in range [0,2]\n",
    "    noise = np.random.normal(mu,math.sqrt(variance), size=(20,)) \n",
    "    y = th5*x**5 + th3*x**3 + th2*x**2 + th1*x + th0 + noise\n",
    "    y_true = th5*x**5 + th3*x**3 + th2*x**2 + th1*x + th0\n",
    "\n",
    "    return(x, y, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kh-xAfIl-O6L"
   },
   "source": [
    "Actual polynomial model parameters and Sample range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUtGRd---O6M"
   },
   "outputs": [],
   "source": [
    "#polynomial weights\n",
    "th0 = 0.2\n",
    "th1 = -1\n",
    "th2 = 0.9\n",
    "th3 = 0.7\n",
    "th5 = -0.2\n",
    "\n",
    "mu, variance = 0, 0.1 # mean and standard deviation of Input noise\n",
    "noise = np.random.normal(mu,math.sqrt(variance), [20,])\n",
    "\n",
    "#Input data range\n",
    "r_start = 0\n",
    "r_end = 2\n",
    "N  = 20\n",
    "step = (r_end - r_start) / N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEq45KZu-O6N"
   },
   "source": [
    "*Assign values to {x, y} set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJI6Y_P8-O6O"
   },
   "outputs": [],
   "source": [
    "x, y, y_true = y_actual_res(th0, th1, th2, th3, th5, step, r_start, r_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ7KTyN5-O6O"
   },
   "source": [
    "Predict Results $\\rightarrow$\n",
    "*Compute parameters from Input data of the $5^{th}$ degree polynomial model with Least Squares method*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "oI7ILlVS-O6P",
    "outputId": "4b75b0fa-3661-4cfa-8148-d58cff07993f"
   },
   "outputs": [],
   "source": [
    "# plotting the points \n",
    "plt.plot(x, y, marker='o',linewidth=0, label = 'experiment input', ) \n",
    "\n",
    "# naming the x axis \n",
    "plt.xlabel('x - axis') \n",
    "# naming the y axis \n",
    "plt.ylabel('y - axis') \n",
    "\n",
    "# giving a title to my graph \n",
    "plt.title('Experiments Input') \n",
    "\n",
    "# function to show the plot \n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parameter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_deg = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Build vector **$X$**: [1  $x$  $x^{2}$  $x^{3}$  $x^{5}$] in range [0,2] of N points to use for trainning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "55b-0Agv6ypO",
    "outputId": "319d2835-5c66-430c-fd4c-bab29574198c"
   },
   "outputs": [],
   "source": [
    "Q_train = np.zeros(shape = (N,poly_deg+1))\n",
    "Q_train[:,0] = 1\n",
    "\n",
    "for i in range(1,poly_deg+1):\n",
    "    Q_train[:,i] = np.power(x,i).reshape((N,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parameter $Θ$ Calculation with train data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta = ((X.T * X)^-1) * Χ.Τ * Υ\n",
    "theta = np.linalg.pinv(Q_train.T.dot(Q_train)).dot(Q_train.T).dot(y)\n",
    "\n",
    "plt.plot(x,Q_train.dot(theta), marker='o',linewidth=0, label='polynomial fit degree =' + str(poly_deg))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H28JNBq-O6U"
   },
   "source": [
    "Calculation of **MSE (Mean Square Error)** of y over the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5sqt1A4k-O6V",
    "outputId": "34906b6d-ebe0-4ff5-8110-7db9a4f2ab80"
   },
   "outputs": [],
   "source": [
    "y_act = y_true\n",
    "y_pred = Q_train.dot(theta)\n",
    "summation = 0  #variable to store the summation of differences\n",
    "n = len(y_act) #finding total number of items in list\n",
    "for i in range(len(y_act)):  #looping through each element of the list\n",
    "    difference = y_act[i] - y_pred[i]  #finding the difference between observed and predicted value\n",
    "    squared_difference = pow(difference,2)  #taking square of the differene \n",
    "    summation = summation + squared_difference  #taking a sum of all the differences\n",
    "MSE = summation/n  #dividing summation by total values to obtain average\n",
    "print (\"The Mean Square Error is: \" , MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVK4H5hJ68KF"
   },
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNThXp-76fOr"
   },
   "outputs": [],
   "source": [
    "#  Problem 1 - ii - Least Squares Regression\n",
    "\n",
    "# importing the required modules \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9wlShJ_r5ly"
   },
   "source": [
    "Below you can see a function to acquire the **y train data** for a <u>N size input vector</u> from a given $5^{th}$ degree polynomial with *random i.i.d. noise samples* originating from a Gaussian distribution added for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zcnYEC5sC6f"
   },
   "outputs": [],
   "source": [
    "def get_y_train(t0, t1, t2, t3, t4, t5, mean, variance, N, x_input):\n",
    "\n",
    "    noise = np.random.normal(mean,math.sqrt(variance), size=(N,)) # An N-size vector of random gaussian noise samples with mean value equal to \"mean\" variable and variance equal to \"variance\" variable\n",
    "    y = t5*x_input**5 + t4*x_input**4 + t3*x_input**3 + t2*x_input**2 + t1*x_input + t0 + noise # The N-size vector of our y_train data\n",
    "\n",
    "    return (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIO4OxloyABR"
   },
   "source": [
    "Below you can see a function to acquire the **y estimates** given x and y train data, the degree of the polynomial model and the size of train data vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYsz3gF_yAvV"
   },
   "outputs": [],
   "source": [
    "def get_y_est(poly_deg, x_train, y_train, N):\n",
    "\n",
    "    Phi = np.zeros(shape = (N,poly_deg+1)) # initialize the Phi matrix with the correspondent dimensions as determined by theory\n",
    "    Phi[:, -1] = 1 # the last column (corrensponding to the fixed term of the polynomian) is set to have 1 as value for all rows\n",
    "\n",
    "    for i in range(0,poly_deg):\n",
    "        Phi[:,i] = np.power(x_train,i+1) # calculating the rest values for Phi matrix as powers of the x_train vector\n",
    "\n",
    "    # calculating the estimates for the thetas according to the theory type for Least Squared method for Generalized Linear Regression (white gaussian noise)\n",
    "    theta = np.linalg.pinv(Phi.T.dot(Phi)).dot(Phi.T).dot(y_train) \n",
    "    y_est = Phi.dot(theta) # calculating y estimates\n",
    "    return (y_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nziVxSr3Govx"
   },
   "source": [
    "The below function calculates the **mean** and **variance** for each point for *exp_num experiments* (noise in train data is variable) and then **plots them along with the curve corresponding to the true model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQUKeq27GsI9"
   },
   "outputs": [],
   "source": [
    "def plot_pol_est_result(pol_deg, N, x_t, exp_num, t0, t1, t2, t3, t4, t5, mean, variance):\n",
    "    experiments = [] # list gathering the results for each experiment\n",
    "    for i in range(0,exp_num):\n",
    "        y_train = get_y_train(t0, t1, t2, t3, t4, t5, mean, variance, N, x_t) # acquire train data for each experiment\n",
    "\n",
    "        experiments.append(get_y_est(pol_deg, x_t, y_train, N)) # append estimates for experiment\n",
    "\n",
    "    ##### Expected Value calculation\n",
    "    E_Y = np.zeros(N)\n",
    "    for i in range(0, exp_num):\n",
    "        E_Y = E_Y + experiments[i]\n",
    "\n",
    "    E_Y = E_Y / exp_num\n",
    "\n",
    "    print('Results for polynomial of degree: ', pol_deg, '\\n')\n",
    "    print('Mean values: \\n')\n",
    "    print(E_Y)\n",
    "    print('\\n')\n",
    "\n",
    "    ##### Variance calculation \n",
    "    var_Y = np.zeros(N)\n",
    "    for i in range(0, exp_num):\n",
    "        var_Y = var_Y + (experiments[i] - E_Y)**2\n",
    "\n",
    "    var_Y = var_Y / exp_num\n",
    "\n",
    "    print('Variance values: \\n')\n",
    "    print(var_Y)\n",
    "    print('\\n')\n",
    "\n",
    "    #### True model calculation\n",
    "    x_true = np.linspace(0, 2, 100) # to simulate continuity we increase the number of points\n",
    "    y_true = t5*x_true**5 + t3*x_true**3 + t2*x_true**2 + t1*x_true + t0 # true model has no noise added\n",
    "\n",
    "    plt.plot(x_true, y_true, '-g', label= 'true_model')\n",
    "\n",
    "    plt.errorbar(x_t, E_Y, marker='o', markersize='2', linewidth=0, yerr= var_Y, elinewidth = 1, capsize = 5, label='polynomial LS regression degree: ' + str(pol_deg))\n",
    "\n",
    "    # naming the x axis \n",
    "    plt.xlabel('x - axis') \n",
    "    # naming the y axis \n",
    "    plt.ylabel('y - axis') \n",
    "\n",
    "    # giving a title to my graph \n",
    "    plt.title('True model vs Polynomial Least Squares Regression') \n",
    "\n",
    "    # function to show the plot \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vy1IKLRVRzl6"
   },
   "source": [
    "Below we utilize the above defined fucntions to run the experiments **at first for a $2^{nd}$ degree** polynomial and **then for a $10^{th}$ degree** one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c3CmGQWsRz6m",
    "outputId": "04e1c22a-e495-4129-bf55-e6d439ef76a2"
   },
   "outputs": [],
   "source": [
    "# polynomial factors of true model used in experiments\n",
    "th0 = 0.2\n",
    "th1 = -1\n",
    "th2 = 0.9\n",
    "th3 = 0.7\n",
    "th5 = -0.2\n",
    "\n",
    "# data range\n",
    "r_start, r_end = 0, 2\n",
    "# number of points\n",
    "points_number  = 20\n",
    "# number of experiments\n",
    "experiments_number = 100\n",
    "# mean, variance values for Gaussian noise\n",
    "noise_mean, noise_variance = 0, 0.1\n",
    "# take \"points_number\" equidistant points in [r_start, r_end]\n",
    "x_train = np.linspace(r_start, r_end, num= points_number)\n",
    "\n",
    "# run experiments for 2nd degree polynomial\n",
    "plot_pol_est_result(2, points_number, x_train, experiments_number, th0, th1, th2, th3, 0, th5, noise_mean, noise_variance)\n",
    "print('\\n')\n",
    "# run experiments for 10th degree polynomial\n",
    "plot_pol_est_result(10, points_number, x_train, experiments_number, th0, th1, th2, th3, 0, th5, noise_mean, noise_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_avWvCInVbge"
   },
   "source": [
    "<u>**Conclusions:**</u>\n",
    "\n",
    "As expected, the **$2^{nd}$ degree** polynomial <u>does not produce a good estimate</u> of the true model which is of **$5^{th}$ degree**. On the other hand, the **$10^{th}$ degree** polynomial <u>fits much better (maybe even overfits)</u> the true model. However, we have to mention that **the variance** of y points is <u>much greater in the second case</u>.\n",
    "\n",
    "**This is justified by the bias-variance dilemma**: *For <u>a fixed number</u> of train points (in our case 20), as the complexity of the estimator is increased the bias becomes smaller, but the variance of the estimates become higher because there are more free parameters to estimate between changing train set*.\n",
    "\n",
    "So for the $2^{nd}$ degree polynomial we have high bias, but generally small variance and for the $10^{th}$ degree polynomial we succeed in minimizing bias, but with the cost of much higher variance for our estimates. **In order to avoid this and minimize both terms, we need to gradually and carefully increase the number of train points and the complexity of the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV0LolO46_PV"
   },
   "source": [
    "### 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbljXfQjcVf5"
   },
   "source": [
    "*Importing required modules*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqDx-QiR7BQV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uFqsv8pcbPg"
   },
   "source": [
    "*Function to generate our Input & \"Experimental\" Results in arrays*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ora-eoNGca4U"
   },
   "outputs": [],
   "source": [
    "def x_y_actual(t0, t1, t2, t3, t5, r_step, r__start, r__end,sig):\n",
    "\n",
    "    _x = np.linspace(r__start, r__end, num= N)               #input range [0,2] including 2 \n",
    "    noise = np.random.normal(mu,math.sqrt(sig), size=(N,))   #adding random noise of input data\n",
    "    _y = t5*_x**5 + t3*_x**3 + t2*_x**2 + t1*_x + t0 + noise \n",
    "    _y_true  = t5*_x**5 + t3*_x**3 + t2*_x**2 + t1*_x + t0 \n",
    "\n",
    "    return(_x, _y, _y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yK2kiLgccsTV"
   },
   "source": [
    "*Actual polynomial model parameters and Sample range*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VB2kF2xqcr-w"
   },
   "outputs": [],
   "source": [
    "#polynomial weights\n",
    "th0 = 0.2\n",
    "th1 = -1\n",
    "th2 = 0.9\n",
    "th3 = 0.7\n",
    "th5 = -0.2\n",
    "\n",
    "#Input data range\n",
    "r_start = 0\n",
    "r_end = 2\n",
    "N  = 20\n",
    "step = (r_end - r_start) / N\n",
    "\n",
    "# mean and standard deviation of Input noise\n",
    "mu, sigma = 0, 0.1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTICTchy9OLs"
   },
   "source": [
    "*Assign values to {x, y} set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFDzez0e9Nyj"
   },
   "outputs": [],
   "source": [
    "x, y , y_true = x_y_actual(th0, th1, th2, th3, th5, step, r_start, r_end, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txcc8Hmr9XQA"
   },
   "source": [
    "Predict Results $\\rightarrow$\n",
    "*Compute parameters from Input data of the $5^{th}$ degree polynomial model with Ridge Regression*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AsU4fYu9mLB"
   },
   "source": [
    "*Parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jy_4_CpA9edd"
   },
   "outputs": [],
   "source": [
    "poly_deg = 5\n",
    "lamda = 0.02   #Optimal value is selected by MSE least value, compputed below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_As9x9ib9wCL"
   },
   "source": [
    " *Build vector **$X$**: [1  $x$  $x^{2}$  $x^{3}$  $x^{5}$] in range [0,2] of N points to use for trainning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eTcRCiG9s7R"
   },
   "outputs": [],
   "source": [
    "Q_train = np.zeros(shape = (N,poly_deg+1))\n",
    "Q_train[:,0] = 1\n",
    "for i in range(1,poly_deg+1):\n",
    "    if i!=4:         #we do not want term x^4 to contribute\n",
    "        Q_train[:,i] = np.power(x,i).reshape((N,))   #[x, x^2, x^3, x^5] of input data x ∈ [0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwUUtK2J94l8"
   },
   "source": [
    "*Parameter $Θ$ Calculation with train data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUUqvWmj97st",
    "outputId": "9f4d403f-bb3c-431c-c9a8-69880f279506"
   },
   "outputs": [],
   "source": [
    "#closed form solution for theta = ((X.T * X + λΙ)^-1) * Χ.Τ * Υ\n",
    "theta = np.linalg.pinv((Q_train.T.dot(Q_train) + lamda*np.eye(poly_deg+1))).dot(Q_train.T).dot(y)\n",
    "print('Theta value is:\\n',str(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRVOa1v1-CPP"
   },
   "source": [
    "*Check MSE behaviour vs different $λ$ values & plot them*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Qh0SvfFS-JM7",
    "outputId": "36b58e64-1b91-4cb3-fc41-4509b71a4ceb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "sample_shp = 500\n",
    "\n",
    "# Initialize arrays\n",
    "mse = np.zeros((sample_shp))\n",
    "mse_better = np.zeros((sample_shp))\n",
    "\n",
    "# Define lamda values range & density\n",
    "l_str = 0\n",
    "l_end = 2.0\n",
    "step_l = (l_end - l_str) / sample_shp\n",
    "lamd_range = np.arange(l_str ,l_end, step_l)  \n",
    "i,m =0, 0\n",
    "for lamda in lamd_range:\n",
    "\n",
    "    #closed form solution for theta = ((X.T * X + λΙ)^-1) * Χ.Τ * Υ\n",
    "    theta = np.linalg.pinv((Q_train.T.dot(Q_train) + lamda*np.eye(poly_deg+1))).dot(Q_train.T).dot(y)\n",
    "\n",
    "    prev = 0 \n",
    "    for k in range(len(y)):\n",
    "    \n",
    "        mse[i] = prev + pow(Q_train.dot(theta)[k] - y_true[k],2)  \n",
    "        prev =  mse[i]\n",
    "\n",
    "    if mse[i] < mse[0]:\n",
    "        mse_better[i] = mse[i]\n",
    "\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "opt = np.argmin(mse)    #index of optimal lamda (least value of MSE)\n",
    "mse_better[ mse_better==0 ] = np.nan\n",
    "\n",
    "# Plot to see the dependency\n",
    "\n",
    "plt.plot(lamd_range, mse_better , 'r|',linewidth=0, label='MSE better than LS for these lamda',markersize=6)\n",
    "plt.plot(lamd_range, mse, 'k.',linewidth=0, label='MSE vs lamda',markersize=2)\n",
    "plt.plot(lamd_range[opt], mse[opt], 'g^',linewidth=0, label='optimal lamda')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZIoBN3Z-Vlz"
   },
   "source": [
    "**Optimal Lamda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdzuzEQr-SMd",
    "outputId": "69e0de89-3de4-49b2-aa0f-ed6c099114bd"
   },
   "outputs": [],
   "source": [
    "found =  mse[opt] - min(mse)\n",
    "print('Ridge regression Method.\\nFor the optimal value of lamda = ', str(lamd_range[opt]),\", the least MSE is= \", str(round(mse[opt],3)))\n",
    "print(\"\\n\\nLeast Squares Method:\\nMSE=\",str((round(mse[0],3))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRfWIbMHd5BL"
   },
   "source": [
    "<u>**Conclusions:**</u>\n",
    "\n",
    "From the graph above we can observe that for different values of lamda, exist some values (noted with red line) for which, MSE value is reduced compared with the MSE using Least Squares method(*λ*=0). Giving that, we can conclude that adding proper value of bias can improve the predictions of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_kirO4W7AKE"
   },
   "source": [
    "### 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cw4yoiGF7E6b"
   },
   "outputs": [],
   "source": [
    "#  Assignment 1 - iv - Full Bayesian Regression (θο equal to true parameter vector)\n",
    "\n",
    "# importing the required module \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKiGX-u1-O6Z"
   },
   "source": [
    "Below you can see a function to acquire the **y train data** for a <u>N size input vector</u> from a given $5^{th}$ degree polynomial with *random i.i.d. noise samples* originating from a Gaussian distribution added for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfdBVu4W-O6a"
   },
   "outputs": [],
   "source": [
    "def get_y_train(t0, t1, t2, t3, t4, t5, mu, variance, N, x):\n",
    "\n",
    "    noise = np.random.normal(mu,math.sqrt(variance), size=(N,))  # An N-size vector of random gaussian noise samples with mean value equal to \"mean\" variable and variance equal to \"variance\" variable\n",
    "    y = t5*x**5 + t4*x**4 + t3*x**3 + t2*x**2 + t1*x + t0 + noise # The N-size vector of our y_train data\n",
    "\n",
    "    return (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwo2fczq-O6b"
   },
   "source": [
    "We use the true parameter vector as the mean $θ_{0}$. We also get 20 random test points in [0, 2] interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "___poR6A-O6b"
   },
   "outputs": [],
   "source": [
    "th = [-1, 0.9, 0.7, 0, -0.2, 0.2] # true paremeter vector\n",
    "random.seed()\n",
    "x_random = np.array([])\n",
    "for i in range(0, 20):\n",
    "    x_random = np.append(x_random, [random.random()*2]) # random.random() return random points in [0, 1] therefore we multiply with 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cC7gIguR-O6c"
   },
   "source": [
    "i) 1st case:\n",
    "   Ν = 20 , $σ_{η}^{2}$ = 0.05, $σ_{θ}^{2}$ = 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIyx_8Nc-O6c"
   },
   "source": [
    "We assign the values which are necessary for this case on the variables sigma_h, N, sigma_theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycoQxMHa-O6d"
   },
   "outputs": [],
   "source": [
    "sigma_h = 0.05\n",
    "N = 20\n",
    "sigma_theta = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztbrih3d-O6d"
   },
   "source": [
    "The true model structure is used in order to construct the **matrix Phi**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVkfPPOU-O6d"
   },
   "outputs": [],
   "source": [
    "x_train = np.linspace(0, 2, num= N) # get N equidistant point in [0, 2] inverval\n",
    "y_train = get_y_train(th[5], th[0], th[1], th[2], th[3], th[4], 0, sigma_h, N, x_train) # get the y train data for the given input and parameters\n",
    "\n",
    "pol_degree = 5\n",
    "\n",
    "Phi = np.zeros(shape = (N,(pol_degree + 1))) # 6 = polynomial degree + 1\n",
    "Phi[:, -1] = 1 # last column, all rows equal to 1 for the fixed term of θ_0\n",
    "\n",
    "phi_x_random = [] # this is going to be used for calculating y estimates for the random points\n",
    "for i in range(0,5):\n",
    "    phi_x_random.append(np.power(x_random,i+1)*th[i]) # φ(x) matrix calculation (actually φ(x) is a vector but here is a matrix because we compute its values for all points in x_random vector)\n",
    "    Phi[:,i] = np.power(x_train,i+1)*th[i] # Φ calculation\n",
    "\n",
    "phi_x_random.append(np.array([1]*x_random.shape[0]))  # append a vector of ones as last element, according to theory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zK0_nT5Q-O6e"
   },
   "source": [
    "We calculate the predicted values of y and we plot them corresponding to the input value x (blue points of the diagram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "eadbTOLa-O6f",
    "outputId": "6828f9d3-3a55-4d3d-9444-c375681b6db4"
   },
   "outputs": [],
   "source": [
    "est_th_y = th + 1 / sigma_h * np.linalg.inv(1 / sigma_theta * np.eye(6) + 1 / sigma_h * Phi.T.dot(Phi)).dot(Phi.T).dot(y_train - Phi.dot(th))\n",
    "\n",
    "# print('μ_θ_y:')\n",
    "# print(est_th_y)\n",
    "# print('\\n')\n",
    "phi_x_random = np.array(phi_x_random) # turn the list into an array\n",
    "\n",
    "est_y = phi_x_random.T.dot(est_th_y) # calculation of the estimates of the model full bayesian inference produce, for the points in x_random vector\n",
    "\n",
    "sigma_y = sigma_h + sigma_h * sigma_theta * phi_x_random.T.dot(np.linalg.inv(sigma_h * np.eye(6) + sigma_theta * Phi.T.dot(Phi))).dot(phi_x_random) # variance - covariance matrix\n",
    "y_var = sigma_y.diagonal()\n",
    "\n",
    "#### True model calculation\n",
    "x_true = np.linspace(0, 2, 100) # to simulate continuity we increase the number of points\n",
    "y_true = th[4]*x_true**5 + th[2]*x_true**3 + th[1]*x_true**2 + th[0]*x_true + th[5] # true model has no noise added\n",
    "\n",
    "plt.errorbar(x_random, est_y, marker= 'o', markersize=2, linewidth=0, yerr= y_var, elinewidth = 1, capsize = 5, label='bayes')\n",
    "plt.plot(x_true, y_true, '-g', linewidth=2, label= 'true_model')\n",
    "\n",
    "# naming the x axis \n",
    "plt.xlabel('x - axis') \n",
    "# naming the y axis \n",
    "plt.ylabel('y - axis') \n",
    "\n",
    "# giving a title to my graph \n",
    "plt.title('True Model vs Full Bayesian Inference Regression') \n",
    "\n",
    "# function to show the plot \n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SI72jcl6-O6g"
   },
   "source": [
    "ii) 2nd case:\n",
    "Ν = 20 , $σ_{η}^{2}$ = 0.05, $σ_{θ}^{2}$ = 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7GmOSfx-O6g"
   },
   "source": [
    "We assign the values which are necessary for this case on the variables sigma_h, N, sigma_theta. The same random points are going to be used in order to extract useful and meaningful conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jY0VQPB-O6h"
   },
   "outputs": [],
   "source": [
    "sigma_h = 0.15\n",
    "N = 20\n",
    "sigma_theta = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oumy-APe-O6i"
   },
   "source": [
    "The true model structure is used in order to construct the **matrix Phi**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CnHdv_s-O6j"
   },
   "outputs": [],
   "source": [
    "x_train = np.linspace(0, 2, num= N)\n",
    "y_train = get_y_train(th[5], th[0], th[1], th[2], th[3], th[4], 0, sigma_h, N, x_train)\n",
    "\n",
    "pol_degree = 5\n",
    "Phi = np.zeros(shape = (N,(pol_degree + 1))) # 6 = polynomial degree + 1\n",
    "Phi[:, -1] = 1\n",
    "\n",
    "phi_x_random = [] # in fact the calculation of phi_x_random could have been avoided, since the paremeters it's depending on did not change, but we recalculate it here for independence of experiments\n",
    "for i in range(0,5):\n",
    "    phi_x_random.append(np.power(x_random,i+1)*th[i])\n",
    "    Phi[:,i] = np.power(x_train,i+1)*th[i]\n",
    "\n",
    "phi_x_random.append(np.array([1]*x_random.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95e78sQmqqT4"
   },
   "source": [
    "We calculate the predicted values of y and we plot them corresponding to the input value x (blue points of the diagram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "_x7nC4ax-O6m",
    "outputId": "b900ac0c-d5a7-447a-e69b-785f8a831c63"
   },
   "outputs": [],
   "source": [
    "est_th_y = th + 1 / sigma_h * np.linalg.inv(1 / sigma_theta * np.eye(6) + 1 / sigma_h * Phi.T.dot(Phi)).dot(Phi.T).dot(y_train - Phi.dot(th))\n",
    "\n",
    "# print('μ_θ_y:')\n",
    "# print(est_th_y)\n",
    "# print('\\n')\n",
    "phi_x_random = np.array(phi_x_random) # turn the list into an array\n",
    "\n",
    "est_y = phi_x_random.T.dot(est_th_y) # calculation of the estimates of the model full bayesian inference produce, for the points in x_random vector\n",
    "\n",
    "\n",
    "# print('μ_y:')\n",
    "# print(est_y)\n",
    "# print('\\n')\n",
    "\n",
    "# calculation of errors/variances for the estimates (variance - covariance matrix)\n",
    "sigma_y = sigma_h + sigma_h * sigma_theta * phi_x_random.T.dot(np.linalg.inv(sigma_h * np.eye(6) + sigma_theta * Phi.T.dot(Phi))).dot(phi_x_random) # variance - covariance matrix\n",
    "# we keep only the elements of the diagonal of the covariance matrix which correspond to the variances we are interested in \n",
    "y_var = sigma_y.diagonal() \n",
    "\n",
    "\n",
    "#### True model calculation\n",
    "x_true = np.linspace(0, 2, 100)\n",
    "y_true = th[4]*x_true**5 + th[2]*x_true**3 + th[1]*x_true**2 + th[0]*x_true + th[5]\n",
    "\n",
    "plt.errorbar(x_random, est_y, marker= 'o', markersize=2, linewidth=0, yerr= y_var, elinewidth = 1, capsize = 5, label='bayes')\n",
    "plt.plot(x_true, y_true, '-g', linewidth=2, label= 'true_model')\n",
    "\n",
    "# naming the x axis \n",
    "plt.xlabel('x - axis') \n",
    "# naming the y axis \n",
    "plt.ylabel('y - axis') \n",
    "\n",
    "# giving a title to my graph \n",
    "plt.title('True Model vs Full Bayesian Inference Regression') \n",
    "\n",
    "# function to show the plot \n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQEH2Mb3-O6n"
   },
   "source": [
    "<u>**Conclusions:**</u>\n",
    "\n",
    "As it is clear the value of N = 20 is large enough in order to have the best predictions. Moreover, we can observe that the larger the noise variance is, the larger the error bars (the variance of estimations/predictions) become."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6cDFpTt7Hc1"
   },
   "source": [
    "### 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tnFo3Je7HFH"
   },
   "outputs": [],
   "source": [
    "#  Problem 1 - v - Full Bayesian Regression (mean vector θο not equal to true parameter vector)\n",
    "\n",
    "# importing the required module \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyGVslqkqRaK"
   },
   "source": [
    "Below you can see a function to acquire the **y train data** for a <u>N size input vector</u> from a given $5^{th}$ degree polynomial with *random i.i.d. noise samples* originating from a Gaussian distribution added for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJUIpBSZqR-R"
   },
   "outputs": [],
   "source": [
    "def get_y_train(t0, t1, t2, t3, t4, t5, mu, variance, N, x):\n",
    "\n",
    "    noise = np.random.normal(mu,math.sqrt(variance), size=(N,)) # An N-size vector of random gaussian noise samples with mean value equal to \"mean\" variable and variance equal to \"variance\" variable\n",
    "    y = t5*x**5 + t4*x**4 + t3*x**3 + t2*x**2 + t1*x + t0 + noise # The N-size vector of our y_train data\n",
    "\n",
    "    return (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQgmXrOaqj9-"
   },
   "source": [
    "Below you can see a function that **implements full bayesian inference** for *N training points* and with *variance sigma_theta for the Gaussian function of the prior*. The **mean vector $θ_{0}$** is <u>following the correct model as for the polynomial degree but with different values from the true model</u>. Finally a plot of the estimates and their errors for the points in the x_random vector is drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5RhkU9BqkPn"
   },
   "outputs": [],
   "source": [
    "def bayesian_inference(N, sigma_theta, x_random):\n",
    "\n",
    "    x_train = np.linspace(0, 2, num= N) # get N equidistant point in [0, 2] inverval\n",
    "    th = [-1, 0.9, 0.7, 0, -0.2, 0.2]  # true paremeter vector\n",
    "    theta_zero = np.array([-10.54, 0.465, 0.0087, 0, -0.093, -0.004]) # get the array from a list of values for mean θ0 vector\n",
    "    sigma_h = 0.05 # variance of the noise gaussian model\n",
    "\n",
    "    y_train = get_y_train(th[5], th[0], th[1], th[2], th[3], th[4], 0, sigma_h, N, x_train) # get the y train data for the given input and parameters\n",
    "\n",
    "    Phi = np.zeros(shape = (N,6)) # 6 = polynomial degree + 1\n",
    "    Phi[:, -1] = 1 # last column, all rows equal to 1 for the fixed term of θ_0\n",
    "\n",
    "    phi_x_random = [] # this is going to be used for calculating y estimates for the random points\n",
    "    for i in range(0,5):\n",
    "        phi_x_random.append(np.power(x_random,i+1)*theta_zero[i]) # φ(x) matrix calculation (actually φ(x) is a vector but here is a matrix because we compute its values for all points in x_random vector)\n",
    "        Phi[:,i] = np.power(x_train,i+1)*theta_zero[i] # Φ calculation\n",
    "\n",
    "    phi_x_random.append(np.array([1]*x_random.shape[0])) # append a vector of ones as last element, according to theory\n",
    "\n",
    "    est_th_y = theta_zero + 1 / sigma_h * np.linalg.inv(1 / sigma_theta * np.eye(6) + 1 / sigma_h * Phi.T.dot(Phi)).dot(Phi.T).dot(y_train - Phi.dot(theta_zero)) # calculation of estimates for θ vector for the given y train data\n",
    "\n",
    "    # print('μ_θ_y:')\n",
    "    # print(est_th_y)\n",
    "    # print('\\n')\n",
    "    phi_x_random = np.array(phi_x_random) # turn the list into an array\n",
    "\n",
    "    est_y = phi_x_random.T.dot(est_th_y) # calculation of the estimates of the model full bayesian inference produce, for the points in x_random vector\n",
    "\n",
    "    # print('μ_y:')\n",
    "    # print(est_y)\n",
    "    # print('\\n')\n",
    "\n",
    "    sigma_y = sigma_h + sigma_h * sigma_theta * phi_x_random.T.dot(np.linalg.inv(sigma_h * np.eye(6) + sigma_theta * Phi.T.dot(Phi))).dot(phi_x_random) # calculation of errors/variances for the estimates (variance - covariance matrix)\n",
    "    y_var = sigma_y.diagonal() # we keep only the elements of the diagonal of the covariance matrix which correspond to the variances we are interested in \n",
    "\n",
    "    #### True model calculation\n",
    "    x_true = np.linspace(0, 2, 100) # to simulate continuity we increase the number of points\n",
    "    y_true = th[4]*x_true**5 + th[2]*x_true**3 + th[1]*x_true**2 + th[0]*x_true + th[5] # true model has no noise added\n",
    "\n",
    "    plt.errorbar(x_random, est_y, marker= 'o', markersize=2, linewidth=0, yerr= y_var, elinewidth = 1, capsize = 5, label='full bayesian inference N: ' + str(N) + ', σ_θ: ' + str(sigma_theta))\n",
    "    plt.plot(x_true, y_true, '-g', linewidth=1, label= 'true_model')\n",
    "\n",
    "    # naming the x axis \n",
    "    plt.xlabel('x - axis') \n",
    "    # naming the y axis \n",
    "    plt.ylabel('y - axis') \n",
    "\n",
    "    # giving a title to my graph \n",
    "    plt.title('True Model vs Full Bayesian Inference Regression') \n",
    "\n",
    "    # function to show the plot \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B24Tjzryqk19"
   },
   "source": [
    "We get **20 random points in [0, 2] interval** and use the same in each experiment for better and more meaningful comparisons and conclusions. Then we run the 4 seperate experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8qCPL9S4qlFn",
    "outputId": "edd6bba6-d0a5-4570-f7b9-37d028fb3c75"
   },
   "outputs": [],
   "source": [
    "random.seed()\n",
    "x_random = np.array([])\n",
    "for i in range(0, 20):\n",
    "    x_random = np.append(x_random, [random.random()*2]) # random.random() return random points in [0, 1] therefore we multiply with 2\n",
    "\n",
    "bayesian_inference(20, 0.1, x_random) # N = 20, sigma_theta = 0.1\n",
    "print('\\n')\n",
    "bayesian_inference(20, 2, x_random) # N = 20, sigma_theta = 2\n",
    "print('\\n')\n",
    "bayesian_inference(500, 0.1, x_random) # N = 500, sigma_theta = 0.1\n",
    "print('\\n')\n",
    "bayesian_inference(500, 2, x_random) # N = 500, sigma_theta = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2IGhviSx9jE"
   },
   "source": [
    "<u>**Conclusions:**</u>\n",
    "\n",
    "*The results we get for the above 4 experiments were the expected ones according to theory.*\n",
    "\n",
    "The first safe comment that we can make is that <u>for a fixed number of points</u>, **as the variance (uncertainty) of the prior increases, we get better fit for**, <u>which is normal in our case</u>: we have adopted a model correct regarding the degree of the polynomial but **we are not certain about our initial estimates for $θ_{0}$** mean vector of the prior. Therefore **greater variance takes this into account and produces better predictions**.\n",
    "\n",
    "Furthermore, **our estimates approximate/fit better the true model curve as the N increases while keeping constant the variance of the prior**. This is also justified, since <u>more training points is natural to give better results for our prediction even with small variance of the prior</u>.\n",
    "\n",
    "In general as we **increase both the number of training points and the variance of the prior, we get better estimation of the true model**. <u>Also, we do not observe significant differences in the errors of the estimates</u>, although it seems to become a little smaller for greater N and slightly greater as $σ_{θ}$ increases (normal according to theory equation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2Rb1qJ87JVk"
   },
   "source": [
    "### 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIt_GHU_7M4_"
   },
   "source": [
    "Expectation Maximization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required module \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Training Data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build  N = 500 sample Data (training data set) in [0,2] that follow the form y = th0 + th1.x + th2.x^2 + th3.x^3 + th5.x^5 (Eq.1)  with sigma^2=0.05.\n",
    "def y_training_set(t0, t1, t2, t3, t5, mu,variance, N_sam, r__start, r__end):\n",
    "\n",
    "    x = np.linspace(r_start, r_end, num= N_sam)     #includes 2 in range [0,2]\n",
    "    noise = np.random.normal(mu,math.sqrt(variance), size=(N_sam,))  \n",
    "    y = t5*x**5 + t3*x**3 + t2*x**2 + t1*x + t0 + noise\n",
    "    \n",
    "    return(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector Parameter (True)\n",
    "th0 = 0.2\n",
    "th1 = -1\n",
    "th2 = 0.9\n",
    "th3 = 0.7\n",
    "th5 = -0.2\n",
    "\n",
    "#data range\n",
    "r_start = 0\n",
    "r_end = 2\n",
    "N  = 500\n",
    "\n",
    "mu, sigma = 0, 0.05 # mean and standard deviation\n",
    "\n",
    "th = [-1, 0.9, 0.7, 0, -0.2, 0.2]  # true paremeter vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = y_training_set(th0, th1, th2, th3, th5, mu,sigma,N, r_start, r_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "x_random = np.array([])\n",
    "for i in range(0, 20):\n",
    "    x_random = np.append(x_random, [random.random()*2])\n",
    "\n",
    "Phi = np.zeros(shape = (N,6)) # 6 = polynomial degree + 1\n",
    "Phi[:, -1] = 1\n",
    "\n",
    "phi_x_random = []\n",
    "for i in range(0,5):\n",
    "    phi_x_random.append(np.power(x_random,i+1)*th[i])\n",
    "    Phi[:,i] = np.power(x,i+1)*th[i]\n",
    "\n",
    "phi_x_random.append(np.array([1]*x_random.shape[0]))\n",
    "phi_x_random = np.array(phi_x_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation-Maximization Repetitions in order to aproximate the values of $σ_{η}$ & $σ_{θ}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define convergence threshold\n",
    "e = 0.0001\n",
    "\n",
    "k = 6 # θ dimensionality\n",
    "\n",
    "a_temp = 1 #sigma-theta^-2\n",
    "b_temp = 1 #sigma-ita^-2\n",
    "\n",
    "A = 0.0   #A capital\n",
    "B  = 0.0   #B capital\n",
    "step = 1\n",
    "\n",
    "while True:  #random range of iretations to make sure it converges. Instead should be a_temp-a_temp(previous)< e & b_temp-b_temp(previous)< e\n",
    "\n",
    "    # Replace the equations of EM \n",
    "    sigma2_th_wrt_y = np.linalg.inv(a_temp * np.eye(Phi.shape[1]) + b_temp * (Phi.T).dot(Phi))   #Σ.θ|y = (αI+bΦ.ΤΦ) ^-1\n",
    "    mi_th_wrt_y = b_temp * ((sigma2_th_wrt_y.dot(Phi.T)).dot(y))   #μ.θ|y = β*Σ.θ|y*Φ.Τ*y\n",
    "\n",
    "    a_tmp_prev = a_temp\n",
    "    b_tmp_prev = b_temp\n",
    "\n",
    "    A = np.linalg.norm(mi_th_wrt_y)**2  + np.matrix.trace(sigma2_th_wrt_y)    # ||μ.θ|y ||^2 + trace(Σ.θ|y)\n",
    "    B  = np.linalg.norm(y-Phi.dot(mi_th_wrt_y))**2 + np.matrix.trace(Phi.dot(sigma2_th_wrt_y).dot(Phi.T))   # ||y-Φμ.θ|y ||^2 + trace(Φ*Σ.θ|y*Φ.Τ)\n",
    "\n",
    "    a_temp = k/A\n",
    "    b_temp = N/B\n",
    "    \n",
    "    differ_a =  abs(a_temp - a_tmp_prev)     # a_temp-a_temp(j-1)< e \n",
    "    differ_b =  abs(b_temp  - b_tmp_prev)    #& b_temp-b_temp(j-1)<e\n",
    "    \n",
    "    print('\\n',step, '. ' , 'σ_{θ} = ' , round((1/a_temp),3), 'σ_{η} = ' , round((1/b_temp),3))\n",
    "    print(\"\\n   Διαφορά από Επανάληψη: \",  round(differ_a,5),   round(differ_b,5) )\n",
    "    if (differ_a < e and differ_b < e):\n",
    "        break\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate final $σ_{η}$ & $σ_{θ}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Σ_θ_y and μ_θ_y for converged a_temp and b_temp\n",
    "sigma2_th_wrt_y = np.linalg.inv(a_temp * np.eye(Phi.shape[1]) + b_temp * (Phi.T).dot(Phi))   #Σ.θ|y = (αI+bΦ.ΤΦ) ^-1\n",
    "mi_th_wrt_y = b_temp * ((sigma2_th_wrt_y.dot(Phi.T)).dot(y))   #μ.θ|y = β*Σ.θ|y*Φ.Τ*y\n",
    "est_y = phi_x_random.T.dot(mi_th_wrt_y)\n",
    "sigma_h = 1/b_temp\n",
    "sigma_theta = 1/a_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Expected points (with uncertainty) together with true model Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_y = sigma_h + sigma_h * sigma_theta * phi_x_random.T.dot(np.linalg.inv(sigma_h * np.eye(6) + sigma_theta * Phi.T.dot(Phi))).dot(phi_x_random) # variance - covariance matrix\n",
    "y_var = sigma_y.diagonal()\n",
    "\n",
    "x_true = np.linspace(0, 2, 100)\n",
    "y_true = th[4]*x_true**5 + th[2]*x_true**3 + th[1]*x_true**2 + th[0]*x_true + th[5]\n",
    "\n",
    "\n",
    "# plt.plot(x_train, est_y, marker= 'o', markersize=5, linewidth=0, label='bayes')\n",
    "plt.errorbar(x_random, est_y, marker= 'o', markersize=2, linewidth=0, yerr= y_var, elinewidth = 1, capsize = 5, label='Expectation-Maxizimation')\n",
    "plt.plot(x_true, y_true, '-g', linewidth=1, label= 'true_model')\n",
    "\n",
    "# naming the x axis \n",
    "plt.xlabel('x - axis') \n",
    "# naming the y axis \n",
    "plt.ylabel('y - axis') \n",
    "\n",
    "# giving a title to my graph \n",
    "plt.title('True Model vs Expectation-Maximization Regression') \n",
    "\n",
    "# function to show the plot \n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation points follow quite closely the model. \n",
    "\n",
    "The prices converged in a small number of repetitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyNBGEi57L9H"
   },
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4AZiyPnX5Ej"
   },
   "source": [
    "**ATTENTION!**\n",
    "\n",
    "<u>In order the code in below cells to work properly, the data set files should be in the same folder with the notebook, as we have provided them to you. Otherwise, you should change the path for the file read command.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSWoM7Ko7buG"
   },
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2qsSQA6riw0"
   },
   "source": [
    "\n",
    "\n",
    "Import modules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88JLXBUXhNjp"
   },
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  \n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVrz7Joqr7-e"
   },
   "source": [
    "Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRIrasQCsnmY"
   },
   "source": [
    "\n",
    "\n",
    "> Genereic functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-TL2izusTgF"
   },
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        file_reader = reader(file)\n",
    "        for row in file_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def most_frequent(List): \n",
    "    counter = 0\n",
    "    num = List[0] \n",
    "      \n",
    "    for i in List: \n",
    "        curr_frequency = List.count(i) \n",
    "        if(curr_frequency> counter): \n",
    "            counter = curr_frequency \n",
    "            num = i \n",
    "  \n",
    "    return num \n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "\n",
    "# Distance of 2 vectors of dataset\n",
    "def find_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "\n",
    "    return sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdxsFIKwstaq"
   },
   "source": [
    "\n",
    "\n",
    ">KNN algorithm help functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ew58Bvnt2pOl"
   },
   "outputs": [],
   "source": [
    " # Locate the most similar neighbors\n",
    "def find_most_freq_neighbor(knn_num, train_set, test_row):\n",
    "    distances = list()\n",
    "    sorted_dist = list()\n",
    "    neighbors = list()\n",
    "  \n",
    "    for i in train_set:\n",
    "        row_dis = find_distance(i, test_row)\n",
    "        distances.append((i, row_dis))\n",
    "  \n",
    "    # get k neighbors with the smallest distance\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    for j in range(knn_num):\n",
    "        neighbors.append(distances[j][0])\n",
    "  \n",
    "    # i want to find in which class belong the majority of the k neighbors\n",
    "    class_of_knn=most_frequent(neighbors)\n",
    "    return class_of_knn[-1]\n",
    "\n",
    "\n",
    "\n",
    " # We need to create a train set & a test set, compare them & check success percentage\n",
    "def compute_knn_success_w_cross_val(dataset, n_folds):  #cross_validation_split\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    " \n",
    "    #   ----------- Fold level --------------------\n",
    "    for r in range(n_folds):\n",
    "        fold = list()\n",
    "\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))  #steal randomly records of the data set & create a group of \"len(dataset) / n_folds\" records\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)  \n",
    "\n",
    "#     -------- level of each Group of each Fold --------------------\n",
    "# Create a train_set, a test_set w/o class specification, & a set of actual class of test set  \n",
    "    succ_perc_sum = 0\n",
    "    for grp in dataset_split:\n",
    "        train_set = list(dataset_split)\n",
    "        train_set.remove(grp)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        actual_test_set = list()\n",
    "\n",
    "#       ------ level of row of each group of each Fold --------------------\n",
    "# Empty class info of test set & keep class info in actual test set\n",
    "        for row in grp:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None               #hide values of test set in order to predict them\n",
    "            actual_test_set.append(list(row))   #keep actual values of test set\n",
    "\n",
    "        # Predict values of test set using training set\n",
    "        for row in test_set:\n",
    "            row[-1] = find_most_freq_neighbor(num_neighbors, train_set, row)\n",
    "\n",
    "#     -------- level of each Group of each Fold --------------------\n",
    "        # Evaluate Prediction vs Reality \n",
    "        model_success_percentage = compare_pred_actual_perc(test_set, actual_test_set)\n",
    "        succ_perc_sum = model_success_percentage +  succ_perc_sum\n",
    "\n",
    "#     -------- level of each Group of each Fold --------------------\n",
    "    tot_sucess_perc = succ_perc_sum / n_folds\n",
    "    return tot_sucess_perc\n",
    "\n",
    "# Compare Actual values wth predicted to get sucess perentage %\n",
    "def compare_pred_actual_perc(prediction, actual):\n",
    "    correct = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if actual[i] == prediction[i]:\n",
    "            correct += 1\n",
    "        success = correct / len(actual) * 100.0\n",
    "    return success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJhDLSiNtPS3"
   },
   "source": [
    "\n",
    "\n",
    ">KNN algorithm \n",
    "---\n",
    "It takes input data with the number of neighbors selected and returns percentage of KNN algorithm success\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yxALBve2o2F"
   },
   "outputs": [],
   "source": [
    "def kNN(x_filename,num_neighbors):\n",
    "    filename = x_filename\n",
    "    x_dataset = load_file(filename)\n",
    "\n",
    "    # convert input from string to numbers\n",
    "    row_len = len(x_dataset[0])\n",
    "    for i in range(row_len-1):    \n",
    "        str_column_to_float(x_dataset, i)\n",
    "\n",
    "    # Replace class names with numbers (like a dictionary: 1 for 'Iris-setosa', 2 for 'Iris-setosa', 3 for 'Iris-virginica')\n",
    "    correlate_dict = dict()\n",
    "    class_names = set([row[-1] for row in x_dataset])\n",
    "    for i, cl_name in enumerate(class_names,1):\n",
    "        correlate_dict[cl_name] = i\n",
    "    for row in x_dataset:\n",
    "        row[-1] = correlate_dict[row[-1]]   #here we substitute class name \"iris-setosa with 1.. etc..\"\n",
    "\n",
    "    # Split dataset to train and test set, copyning the actual va;ues of test set in actual_test_set\n",
    "    model_success_percentage = compute_knn_success_w_cross_val(x_dataset, 8)   #for k-fold of cross valid, we set k=8\n",
    "    return model_success_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBy-XsvA3Dap"
   },
   "source": [
    "*Play with number of neighbors and Datasets*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdARGREK5TSt"
   },
   "source": [
    "\n",
    "\n",
    "> Pima Indians Diabetes (try KNN from 2 until 200 neighbors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "Lo6oFrh53om5",
    "outputId": "2b8e59af-7948-4876-f9bf-0dc99d9a7c49"
   },
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "perc_list = list()\n",
    "nn_for_max_propab = 0\n",
    "max_prop = 0.0\n",
    "\n",
    "# Problem parameters\n",
    "r_nn_str= 1\n",
    "r_nn_end= 200\n",
    "_filename = 'pima-indians-diabetes.data'\n",
    "\n",
    "for num_neighbors in range (r_nn_str,r_nn_end):\n",
    "    perc_list.append(kNN(_filename, num_neighbors))\n",
    "\n",
    "\n",
    "# Optimal number of neighbors\n",
    "nn_for_max_propab = np.argmax(perc_list)\n",
    "max_prop = perc_list[nn_for_max_propab]\n",
    "\n",
    "print('Maximum percentage of Prediction for this dataset: ', str(round(max_prop,2)), '% using ', str(nn_for_max_propab), ' neighbors' )\n",
    "\n",
    "neib = range (r_nn_str,r_nn_end)\n",
    "\n",
    "# x & y axis naming\n",
    "plt.xlabel('neighbor number of selection') \n",
    "plt.ylabel('percentage of model KNN success') \n",
    "\n",
    "# title & design of graph \n",
    "plt.title(_filename) \n",
    "plt.plot(neib, perc_list, marker='o',linewidth=0) \n",
    "\n",
    "# function to show the plot \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XU0PNTE5dfi"
   },
   "source": [
    "\n",
    "\n",
    ">Iris flowers *(& pima-indians-diabetes at same knn range for compare)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "1_q-jRYS5lbD",
    "outputId": "dc4962ab-e548-410a-9f10-7a09fce51ea7"
   },
   "outputs": [],
   "source": [
    "_filename2 = 'iris.data'\n",
    "_filename1 = 'pima-indians-diabetes.data'\n",
    "_filenames = (_filename1, _filename2)\n",
    "\n",
    "\n",
    "for idx, filename in enumerate(_filenames):\n",
    "    perc_list = list()\n",
    "    nn_for_max_propab = 0\n",
    "\n",
    "    for num_neighbors in range (2,100):\n",
    "        perc_list.append(kNN(filename, num_neighbors))\n",
    "\n",
    "    neib = range (2,100)\n",
    "\n",
    "    if idx%2 == 1:\n",
    "        colr= 'g'\n",
    "    else:\n",
    "        colr= 'b' \n",
    "\n",
    "    nn_for_max_propab = np.argmax(perc_list)\n",
    "    max_prop = perc_list[nn_for_max_propab]\n",
    "    print('Maximum percentage of Prediction for this dataset: ', str(round(max_prop,2)), '% using ', str(nn_for_max_propab), ' neighbors' )\n",
    " \n",
    "\n",
    "    # Plot Results\n",
    "    plt.plot(neib, perc_list, marker='o',linewidth=0, color= colr, label = filename) \n",
    "\n",
    "    #  x & y axis naming\n",
    "    plt.xlabel('neighbor number selection') \n",
    "    plt.ylabel('percentage of model KNN success') \n",
    "\n",
    "    # title of graph \n",
    "    plt.title('KNN Model for 2 datasets') \n",
    "\n",
    "    # functions to show the plot \n",
    "    plt.legend()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXBUb6cS7ez_"
   },
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aujWBOiBX5El"
   },
   "outputs": [],
   "source": [
    "#  Problem 2 - ii - Probability Density Function calculation for different assumptions\n",
    "\n",
    "# importing the required modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "df = pd.read_csv('pima-indians-diabetes.data', delimiter=',', encoding='ISO-8859–1', names=['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09d10p0R7xI8"
   },
   "source": [
    "#### 2.2.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption:** \"*Pdfs are gaussian, with diagonal covariance matrices. Means and covariance matrices of the pdfs are estimated using Maximum Likelihood from the available data.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we calculate the mean and covariance of the samples belonging in **diabetes class ('Y' = 1)** and then we use these values to calculate the estimation for the correspondent pdf, according to the below equation:\n",
    "\n",
    "We calculate the mean and the covariance from the next math formulas:\n",
    "  <br>\n",
    "  $μ = \\frac{1}{Ν} \\sum{x_i}$\n",
    "  <br>\n",
    "  $Σ = \\frac{1}{Ν} \\sum{(x_n-μ)^Τ(x_n-μ)}$\n",
    "  \n",
    "The proof of these formulas are given as a pdf with the submitted .ipynb file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diabetes = np.array([])\n",
    "features = df.keys() # take the feature keys\n",
    "features = features.drop('Y') # drop key corresponding to the class value\n",
    "for key in features:\n",
    "    # calculate the mean for each feature by summing all values and dividing with their number\n",
    "    mean_diabetes = np.append(mean_diabetes, [df[key][df['Y'] == 1].sum()/len(df[key][df['Y'] == 1])])\n",
    "\n",
    "print('mean for diabetes class\\n')\n",
    "print(mean_diabetes)\n",
    "print('\\n')\n",
    "\n",
    "dimension = len(features) # dimensionality of the problem\n",
    "variance_diabetes = np.array([]) # initialize of the variance array\n",
    "covariance_diabetes = np.zeros(shape=(dimension, dimension)) # covariance matrix initialization\n",
    "N_diabetes = len(df[features[1]][df['Y'] == 1]) # the length of dataset for diabetes ('Υ' == 1)\n",
    "buffer = 0 # initialization of a buffer for the calculation of sigma\n",
    "for i in range(0, dimension):\n",
    "    # calculate (co)variance following the theory equation\n",
    "    buffer = buffer + ((df[features[i]][df['Y'] == 1] - mean_diabetes[i]).T).dot(df[features[i]][df['Y'] == 1] - mean_diabetes[i])\n",
    "\n",
    "sigma = buffer/(dimension*N_diabetes)\n",
    "\n",
    "for i in range(0, dimension):\n",
    "    covariance_diabetes[i][i] = sigma\n",
    "\n",
    "print('covariance matrix for diabetes class\\n')\n",
    "print(covariance_diabetes)\n",
    "print('\\n')\n",
    "\n",
    "df_feat_only_diabetes = df.drop('Y', axis=1)[df['Y'] == 1]\n",
    "features_array_diabetes = df_feat_only_diabetes.to_numpy()\n",
    "\n",
    "# the below variables are just intermediate result for the pdf equation, in order to keep it readable\n",
    "buffer_array = features_array_diabetes-mean_diabetes\n",
    "deter_cov_diab = np.linalg.det(covariance_diabetes)\n",
    "cov_diab_inv = np.linalg.inv(covariance_diabetes)\n",
    "\n",
    "# apply the equation from theory to calculate the pdf estimates\n",
    "pdf_diabetes = (1 / ( (2*math.pi)**(dimension/2) * (deter_cov_diab**(1/2)))) * np.exp((-1/2)*(buffer_array.dot(cov_diab_inv)).dot(buffer_array.T))\n",
    "\n",
    "# We avoid using a for loop and use the matrix multiplication because it is optimized in python.\n",
    "pdf_diabetes = pdf_diabetes.diagonal()\n",
    "\n",
    "print('pdf estimation for diabetes class\\n')\n",
    "print(pdf_diabetes)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we calculate the mean and covariance matrix of the samples belonging in **non diabetes class ('Y' = 0)** and then we use these values to calculate the estimation for the correspondent pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_non_diabetes = np.array([])\n",
    "# features array has already been computed in above cell\n",
    "features = df.keys()\n",
    "features = features.drop('Y')\n",
    "for key in features:\n",
    "    # calculate the mean for each feature by summing all values and dividing with their number\n",
    "    mean_non_diabetes = np.append(mean_non_diabetes, [df[key][df['Y'] == 0].sum()/len(df[key][df['Y'] == 0])])\n",
    "\n",
    "print('mean for non diabetes class\\n')\n",
    "print(mean_non_diabetes)\n",
    "print('\\n')\n",
    "\n",
    "dimension = len(features)\n",
    "variance_non_diabetes = np.array([])\n",
    "covariance_non_diabetes = np.zeros(shape=(dimension, dimension))\n",
    "N_non_diabetes = len(df[features[1]][df['Y'] == 0])\n",
    "buffer = 0\n",
    "for i in range(0, dimension):\n",
    "   # calculate (co)variance for each element of the matrix, following the theory equation\n",
    "   buffer = buffer + ((df[features[i]][df['Y'] == 0] - mean_non_diabetes[i]).T).dot(df[features[i]][df['Y'] == 0] - mean_non_diabetes[i])\n",
    "\n",
    "sigma = buffer/(dimension*N_non_diabetes)\n",
    "\n",
    "\n",
    "for i in range(0, dimension):\n",
    "    covariance_non_diabetes[i][i] = sigma\n",
    "\n",
    "print('covariance_diabetes\\n')\n",
    "print(covariance_non_diabetes)\n",
    "print('\\n')\n",
    "\n",
    "df_feat_only_non_diabetes = df.drop('Y', axis=1)[df['Y'] == 0]\n",
    "features_array_non_diabetes = df_feat_only_non_diabetes.to_numpy()\n",
    "# the below variables are just intermediate result for the pdf equation, in order to keep it readable\n",
    "buffer_array = features_array_non_diabetes-mean_non_diabetes\n",
    "deter_cov_diab = np.linalg.det(covariance_non_diabetes)\n",
    "cov_diab_inv = np.linalg.inv(covariance_non_diabetes)\n",
    "\n",
    "# apply the equation from theory to calculate the pdf estimates\n",
    "pdf_non_diabetes = (1 / ( (2*math.pi)**(dimension/2) * (deter_cov_diab**(1/2)))) * np.exp((-1/2)*(buffer_array.dot(cov_diab_inv)).dot(buffer_array.T))\n",
    "\n",
    "#We avoid using a for loop and use the matrix multiplication because it is optimized in python.\n",
    "pdf_non_diabetes = pdf_non_diabetes.diagonal()\n",
    "\n",
    "print('pdf estimation for non diabetes class\\n')\n",
    "print(pdf_non_diabetes)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Goodness of fit measument**</u>\n",
    "\n",
    "In order to calculate **AIC and BIC criterion** values for each class, we calculate firstly log-likelihood L(θ). For the assumption used for pdfs *k* (the number of estimated parameters for our model) is 44 => <u>8 values for means for each $X_{i}$ + 8*(8+1)/2 values because the covariance matrix is symmetric so we have half table plus diagonal</u>. Finally, regarding AIC the **equation for small sample size** is used, since N/k is less than 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2.a\n",
    "## k = 8 values for means for each Xi + 8x(8+1)/2 values because the covariance matrix is symmetric so we have half table plus diagonal\n",
    "k = 8 + 8*9/2\n",
    "\n",
    "print(-2*np.log(pdf_diabetes).sum())\n",
    "AIC_diabetes = -2*np.log(pdf_diabetes).sum() + 2 * k + (2*k+1) / (N_diabetes-k-1)\n",
    "BIC_diabetes = -2*np.log(pdf_diabetes).sum() + k * math.log(N_diabetes)\n",
    "print('AIC_diabetes: ', AIC_diabetes)\n",
    "print('BIC_diabetes: ', BIC_diabetes)\n",
    "AIC_non_diabetes = -2*np.log(pdf_non_diabetes).sum() + 2 * k + (2*k+1) / (N_non_diabetes-k-1)\n",
    "BIC_non_diabetes = -2*np.log(pdf_non_diabetes).sum() + k * math.log(N_non_diabetes)\n",
    "print('AIC_non_diabetes: ', AIC_non_diabetes)\n",
    "print('BIC_non_diabetes: ', BIC_non_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnonK-Uf72_n"
   },
   "source": [
    "#### 2.2.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uvTuVBaX5En"
   },
   "source": [
    "**Assumption:** \"*Pdfs are gaussian, with non-diagonal covariance matrices. Means and covariance matrices of the pdfs are estimated using Maximum Likelihood from the available data.*\""
   ]
  },
  {
   "attachments": {
    "pdf_non_diagonal_cov_matrices.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "id": "wBF3o2poX5Eo"
   },
   "source": [
    "In the next cell we calculate the mean and covariance matrix of the samples belonging in **diabetes class ('Y' = 1)** and then we use these values to calculate the estimation for the correspondent pdf, according to the below equation:\n",
    "\n",
    "![pdf_non_diagonal_cov_matrices.jpg](attachment:pdf_non_diagonal_cov_matrices.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGiXXVEY77d5",
    "outputId": "bb188fec-405a-4ea7-8706-5ac72c4ea011"
   },
   "outputs": [],
   "source": [
    "mean_diabetes = np.array([])\n",
    "features = df.keys() # take the feature keys\n",
    "features = features.drop('Y') # drop key corresponding to the class value\n",
    "for key in features:\n",
    "    # calculate the mean for each feature by summing all values and dividing with their number\n",
    "    mean_diabetes = np.append(mean_diabetes, [df[key][df['Y'] == 1].sum()/len(df[key][df['Y'] == 1])])\n",
    "\n",
    "print('means matrix for diabetes class\\n')\n",
    "print(mean_diabetes)\n",
    "print('\\n')\n",
    "\n",
    "dimension = len(features) # dimensionality of the problem\n",
    "covariance_diabetes = np.zeros(shape=(dimension, dimension)) # covariance matrix initialization\n",
    "for i in range(0, dimension):\n",
    "    for j in range(0, dimension):\n",
    "        # calculate (co)variance for each element of the matrix, following the theory equation\n",
    "        buffer = (df[features[i]][df['Y'] == 1] - mean_diabetes[i])*(df[features[j]][df['Y'] == 1] - mean_diabetes[j])\n",
    "        covariance_diabetes[i][j] = buffer.sum()/len(buffer)\n",
    "\n",
    "print('covariance matrix for diabetes class\\n')\n",
    "print(covariance_diabetes)\n",
    "print('\\n')\n",
    "\n",
    "df_feat_only_diabetes = df.drop('Y', axis=1)[df['Y'] == 1] # get series without class feature, only for diabetes class\n",
    "features_array_diabetes = df_feat_only_diabetes.to_numpy() # turn series to an array\n",
    "\n",
    "# the below variables are just intermediate result for the pdf equation, in order to keep it readable\n",
    "buffer_array = features_array_diabetes-mean_diabetes\n",
    "deter_cov_diab = np.linalg.det(covariance_diabetes)\n",
    "cov_diab_inv = np.linalg.inv(covariance_diabetes)\n",
    "\n",
    "# apply the equation from theory to calculate the pdf estimates\n",
    "pdf_diabetes = (1 / ( (2*math.pi)**(dimension/2) * (deter_cov_diab**(1/2)))) * np.exp((-1/2)*(buffer_array.dot(cov_diab_inv)).dot(buffer_array.T))\n",
    "\n",
    "# because we introduce the whole matrix buffer_array in the equation and not point by point, we calculate more results,\n",
    "# most of them not having actual meaning. Only the diagonal values are the actual pdfs.\n",
    "# however we avoid using a for loop and use the matrix multiplication because it is optimized in python.\n",
    "pdf_diabetes = pdf_diabetes.diagonal()\n",
    "print('pdf estimation for diabetes class\\n')\n",
    "print(pdf_diabetes)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DZt6kxsX5Es"
   },
   "source": [
    "In the next cell we calculate the mean and covariance matrix of the samples belonging in **non diabetes class ('Y' = 0)** and then we use these values to calculate the estimation for the correspondent pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3TuY2QDCX5Es",
    "outputId": "ac699d98-480c-4752-c448-099a63349f49"
   },
   "outputs": [],
   "source": [
    "mean_non_diabetes = np.array([])\n",
    "# features array has already been computed in above cell\n",
    "for key in features:\n",
    "    # calculate the mean for each feature by summing all values and dividing with their number\n",
    "    mean_non_diabetes = np.append(mean_non_diabetes, [df[key][df['Y'] == 0].sum()/len(df[key][df['Y'] == 0])])\n",
    "\n",
    "print('means matrix for non diabetes class\\n')\n",
    "print(mean_non_diabetes)\n",
    "print('\\n')\n",
    "\n",
    "covariance_non_diabetes = np.zeros(shape=(dimension, dimension)) # covariance matrix initialization\n",
    "for i in range(0, dimension):\n",
    "    for j in range(0, dimension):\n",
    "        # calculate (co)variance for each element of the matrix, following the theory equation\n",
    "        buffer = (df[features[i]][df['Y'] == 0] - mean_diabetes[i])*(df[features[j]][df['Y'] == 0] - mean_diabetes[j])\n",
    "        covariance_non_diabetes[i][j] = buffer.sum()/len(buffer)\n",
    "\n",
    "print('covariance matrix for non diabetes class\\n')\n",
    "print(covariance_non_diabetes)\n",
    "print('\\n')\n",
    "\n",
    "df_feat_only_non_diabetes = df.drop('Y', axis=1)[df['Y'] == 0] # get series without class feature, only for diabetes class\n",
    "features_array_non_diabetes = df_feat_only_non_diabetes.to_numpy() # turn series to an array\n",
    "\n",
    "# the below variables are just intermediate result for the pdf equation, in order to keep it readable\n",
    "buffer_array = features_array_non_diabetes-mean_non_diabetes\n",
    "deter_cov_non_diab = np.linalg.det(covariance_diabetes)\n",
    "cov_non_diab_inv = np.linalg.inv(covariance_non_diabetes)\n",
    "\n",
    "# apply the equation from theory to calculate the pdf estimates\n",
    "pdf_non_diabetes = (1 / ( (2*math.pi)**(dimension/2) * (deter_cov_non_diab**(1/2)))) * np.exp((-1/2)*(buffer_array.dot(cov_non_diab_inv)).dot(buffer_array.T))\n",
    "\n",
    "# as already has been stated, because we introduce the whole matrix buffer_array in the equation and not point by point, we calculate more results,\n",
    "# most of them not having actual meaning. Only the diagonal values are the actual pdfs.\n",
    "# however we avoid using a for loop and use the matrix multiplication because it is optimized in python.\n",
    "pdf_non_diabetes = pdf_non_diabetes.diagonal()\n",
    "print('pdf estimation for non diabetes class\\n')\n",
    "print(pdf_non_diabetes)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ejB9GYoX5Et"
   },
   "source": [
    "<u>**Goodness of fit measument**</u>\n",
    "\n",
    "In order to calculate **AIC and BIC criterion** values for each class, we calculate firstly log-likelihood L(θ). For the assumption used for pdfs *k* (the number of estimated parameters for our model) is 44 => <u>8 values for means for each $X_{i}$ + 8*(8+1)/2 values because the covariance matrix is symmetric so we have half table plus diagonal</u>. Finally, regarding AIC the **equation for small sample size** is used, since N/k is less than 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFM8rIXsX5Et",
    "outputId": "43ad827d-30c2-4584-d2d3-37685db455f9"
   },
   "outputs": [],
   "source": [
    "## k = 8 values for means for each Xi + 8x(8+1)/2 values because the covariance matrix is symmetric so we have half table plus diagonal\n",
    "k = 8 + 8*9/2\n",
    "N_diabetes = len(features_array_diabetes[:,0])\n",
    "N_non_diabetes = len(features_array_non_diabetes[:,0])\n",
    "\n",
    "diabetes_loglikelihood = np.log(pdf_diabetes).sum()\n",
    "AIC_diabetes = -2*diabetes_loglikelihood + 2 * k + (2*k+1) / (N_diabetes-k-1)\n",
    "BIC_diabetes = -2*diabetes_loglikelihood + k * math.log(N_diabetes)\n",
    "print('AIC_diabetes: ', AIC_diabetes)\n",
    "print('BIC_diabetes: ', BIC_diabetes)\n",
    "\n",
    "non_diabetes_loglikelihood = np.log(pdf_non_diabetes).sum()\n",
    "AIC_non_diabetes = -2*non_diabetes_loglikelihood + 2 * k + (2*k+1) / (N_non_diabetes-k-1)\n",
    "BIC_non_diabetes = -2*non_diabetes_loglikelihood + k * math.log(N_non_diabetes)\n",
    "print('AIC_non_diabetes: ', AIC_non_diabetes)\n",
    "print('BIC_non_diabetes: ', BIC_non_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVjLC4YH78GQ"
   },
   "source": [
    "#### 2.2.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption:** \"*Components of the feature vectors are mutually statistically independent (the usual naïve Bayes approach). Marginal Pdfs are gaussian, with parameters (mean, variance) estimated using Maximum Likelihood from the available data.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we calculate **the marginal pdf for each feature** of the samples belonging in **diabetes class ('Y' = 1)** and then we calculate the estimation for the correspondent pdf as the product of the marginal pdfs (naive bayes approach).\n",
    "\n",
    "For the computation of the marginal pdfs we use Maximum Likelihood Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1e85M0A79VL"
   },
   "outputs": [],
   "source": [
    "df_feat_only_diabetes = df.drop('Y', axis=1)[df['Y'] == 1]\n",
    "features = df.keys()\n",
    "features = features.drop('Y')\n",
    "features_array_diabetes = df_feat_only_diabetes.to_numpy()\n",
    "marginal_pdfs_diab = [] #initialize an array for every marginal pdf for diabetes\n",
    "\n",
    "#for every feature we calculate the mean and the variance for diabetes\n",
    "for i in range(0, dimension):\n",
    "    mean_diabetes_feature = 0\n",
    "    N_diabetes = len(df[features[i]][df['Y'] == 1])\n",
    "\n",
    "    buffer = np.zeros(N_diabetes)\n",
    "    sum_feature = 0\n",
    "    sum_variance_feature = 0\n",
    "    variance_feature = 0\n",
    "    \n",
    "    #we calculate the mean for every feature according to the math formulas\n",
    "    mean_diabetes_feature =  df[features[i]][df['Y'] == 1].sum()/len(df[features[i]][df['Y'] == 1])\n",
    "    \n",
    "    print(\"mean_feature\")\n",
    "    print(mean_diabetes_feature)\n",
    "    buffer_marg = 0\n",
    "    features_diabetes = df[features[i]][df['Y'] == 1]\n",
    "    \n",
    "    for xi in features_diabetes:\n",
    "        buffer_marg = buffer_marg + (xi - mean_diabetes_feature)*(xi - mean_diabetes_feature)\n",
    "    \n",
    "    #we calculate the variance for every feature according to the math formulas\n",
    "    variance_diabetes = buffer_marg/(N_diabetes)\n",
    "    print(\"variance_diabetes\")\n",
    "    print(variance_diabetes)\n",
    "  \n",
    "    \n",
    "    buffer_array = df[features[i]][df['Y'] == 1]-mean_diabetes_feature\n",
    "    buffer = (1 / ( (2*math.pi)**(1/2) * (variance_diabetes**(1/2)))) * np.exp((-1/2)*(buffer_array**2)/(variance_diabetes))\n",
    " \n",
    "    marginal_pdfs_diab.append(buffer) #append the new marginal pdf in the array\n",
    "    \n",
    "pdf_diabetes_ml = np.ones(marginal_pdfs_diab[0].shape) # initialization for pdf calculation\n",
    "\n",
    "#in order to calculate the total pdf we multiply the pdfs of every marginal pdf bacause of the Naive Bayes\n",
    "for marg_pdf in marginal_pdfs_diab:\n",
    "    pdf_diabetes_ml = pdf_diabetes_ml * marg_pdf\n",
    "\n",
    "print('pdf estimation for diabetes class (naive bayes using ML)\\n')\n",
    "print(pdf_diabetes_ml)\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_only_non_diabetes = df.drop('Y', axis=1)[df['Y'] == 0]\n",
    "features = df.keys()\n",
    "features = features.drop('Y')\n",
    "features_array_diabetes = df_feat_only_non_diabetes.to_numpy()\n",
    "marginal_pdfs_non_diab = [] #initialize an array for every marginal pdf for non diabetes\n",
    "\n",
    "#for every feature we calculate the mean and the variance for diabetes\n",
    "for i in range(0, dimension):\n",
    "    mean_non_diabetes_feature = 0\n",
    "    N_non_diabetes = len(df[features[i]][df['Y'] == 0])\n",
    "\n",
    "    buffer = np.zeros(N_non_diabetes)\n",
    "    sum_feature = 0\n",
    "    sum_variance_feature = 0\n",
    "    variance_non_feature = 0\n",
    "    \n",
    "    #we calculate the mean for every feature according to the math formulas\n",
    "    mean_non_diabetes_feature =  df[features[i]][df['Y'] == 0].sum()/len(df[features[i]][df['Y'] == 0])\n",
    "    \n",
    "\n",
    "    print(\"mean_feature\")\n",
    "    print(i)\n",
    "    print(mean_non_diabetes_feature)\n",
    "    buffer_marg = 0\n",
    "    features_diabetes = df[features[i]][df['Y'] == 0]\n",
    "    for xi in features_diabetes:\n",
    "        buffer_marg = buffer_marg + (xi - mean_non_diabetes_feature)*(xi - mean_non_diabetes_feature)\n",
    "\n",
    "    variance_non_feature = buffer_marg/(N_non_diabetes)\n",
    "    print(\"variance_diabetes\")\n",
    "    print(variance_non_feature)\n",
    "  \n",
    "    \n",
    "    buffer_array = df[features[i]][df['Y'] == 0]-mean_non_diabetes_feature\n",
    "    buffer = (1 / ( (2*math.pi)**(1/2) * (variance_non_feature**(1/2)))) * np.exp((-1/2)*(buffer_array**2)/(variance_non_feature))\n",
    " \n",
    "    marginal_pdfs_non_diab.append(buffer)\n",
    "    \n",
    "pdf_non_diabetes_ml = np.ones(marginal_pdfs_non_diab[0].shape) # initialization for pdf calculation\n",
    "\n",
    "#in order to calculate the total pdf we multiply the pdfs of every marginal pdf bacause of the Naive Bayes\n",
    "for marg_pdf in marginal_pdfs_non_diab:\n",
    "    pdf_non_diabetes_ml = pdf_non_diabetes_ml * marg_pdf\n",
    "\n",
    "print('pdf estimation for non diabetes class (naive bayes using ML)\\n')\n",
    "print(pdf_non_diabetes_ml)\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Goodness of fit measument**</u>\n",
    "\n",
    "In order to calculate **AIC and BIC criterion** values for each class, we calculate firstly log-likelihood L(θ). For the assumption used for pdfs *k* (the number of estimated parameters for our model) is 16 => <u>there are 8 means and 8 covariances because of the features</u>. Finally, regarding AIC the **equation for small sample size** is used, since N/k is less than 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2.c\n",
    "## k = 8 values for means for each Xi + 8x(8+1)/2 values because the covariance matrix is symmetric so we have half table plus diagonal\n",
    "k = 2*8\n",
    "\n",
    "print(-2*np.log(pdf_diabetes_ml).sum())\n",
    "AIC_diabetes = -2*np.log(pdf_diabetes_ml).sum() + 2 * k + (2*k+1) / (N_diabetes-k-1)\n",
    "BIC_diabetes = -2*np.log(pdf_diabetes_ml).sum() + k * math.log(N_diabetes)\n",
    "print('AIC_diabetes: ', AIC_diabetes)\n",
    "print('BIC_diabetes: ', BIC_diabetes)\n",
    "AIC_non_diabetes = -2*np.log(pdf_non_diabetes_ml).sum() + 2 * k + (2*k+1) / (N_non_diabetes-k-1)\n",
    "BIC_non_diabetes = -2*np.log(pdf_non_diabetes_ml).sum() + k * math.log(N_non_diabetes)\n",
    "print('AIC_non_diabetes: ', AIC_non_diabetes)\n",
    "print('BIC_non_diabetes: ', BIC_non_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT9glXC679y8"
   },
   "source": [
    "#### 2.2.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVw1NWk5X5Ex"
   },
   "source": [
    "**Assumption:** \"*Components of the feature vectors are mutually statistically independent (the usual naïve Bayes approach). Marginal pdfs are computed using 1-d Parzen windows with gaussian kernels. Take the width h of each window equal to the square root of the number of patterns in the available data.*\""
   ]
  },
  {
   "attachments": {
    "marginal_pdf_parzen.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "id": "XIVm1dhIX5Ex"
   },
   "source": [
    "In the next cell we calculate **the marginal pdf for each feature** of the samples belonging in **diabetes class ('Y' = 1)** and then we calculate the estimation for the correspondent pdf as the product of the marginal pdfs (naive bayes approach).\n",
    "\n",
    "For the computation of the marginal pdfs we use <u>1-d Parzen windows with gaussian kernels, which have standard deviation equal to the width *h* of the window and mean value each sample point</u>, according to the below equation (we replace σ with h):\n",
    "\n",
    "![marginal_pdf_parzen.jpg](attachment:marginal_pdf_parzen.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v94BZUhb7f1h",
    "outputId": "d4e773f3-61b0-4755-ef76-4fe1b0dbec12"
   },
   "outputs": [],
   "source": [
    "N_diabetes = len(features_array_diabetes[:,0]) # number of samples\n",
    "h_diabetes = math.sqrt(N_diabetes) # set the window width equal to the square root of number of samples for class\n",
    "\n",
    "marginal_pdfs_diab = []\n",
    "for i in range(0, dimension):\n",
    "    buffer = np.zeros(N_diabetes)\n",
    "    for xi in features_array_diabetes[:][i]:\n",
    "        # the below buffer is used for summation of the gaussians (equation from theory)\n",
    "        buffer = buffer + (1 / (math.sqrt(2*math.pi)*h_diabetes)) * np.exp((-1 / (2*(h_diabetes**2)))*(features_array_diabetes[:,i] - xi)**2)\n",
    "    buffer = buffer / N_diabetes\n",
    "    marginal_pdfs_diab.append(buffer)\n",
    "\n",
    "pdf_diabetes_parzen = np.ones(marginal_pdfs_diab[0].shape) # initialization for pdf calculation\n",
    "for marg_pdf in marginal_pdfs_diab:\n",
    "    pdf_diabetes_parzen = pdf_diabetes_parzen * marg_pdf # calculate product repeatedly\n",
    "\n",
    "print('pdf estimation for diabetes class (naive bayes using parzen windows)\\n')\n",
    "print(pdf_diabetes_parzen)\n",
    "print('\\n')"
   ]
  },
  {
   "attachments": {
    "marginal_pdf_parzen.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "id": "hQor5685X5Ey"
   },
   "source": [
    "In the next cell we calculate **the marginal pdf for each feature** of the samples belonging in **non diabetes class ('Y' = 0)** and then we calculate the estimation for the correspondent pdf as the product of the marginal pdfs (naive bayes approach).\n",
    "\n",
    "For the computation of the marginal pdfs we use <u>1-d Parzen windows with gaussian kernels, which have standard deviation equal to the width *h* of the window and mean value each sample point</u>, according to the below equation (we replace σ with h):\n",
    "\n",
    "![marginal_pdf_parzen.jpg](attachment:marginal_pdf_parzen.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNly2m3CX5Ez",
    "outputId": "5a163879-073c-4392-fb26-c2c8723b9c2c"
   },
   "outputs": [],
   "source": [
    "N_non_diabetes = len(features_array_non_diabetes[:,0]) # number of samples\n",
    "h_non_diabetes = math.sqrt(N_non_diabetes) # set the window width equal to the square root of number of samples for class\n",
    "\n",
    "marginal_pdfs_non_diab = []\n",
    "for i in range(0, dimension):\n",
    "    buffer = np.zeros(N_non_diabetes)\n",
    "    for xi in features_array_non_diabetes[:][i]:\n",
    "        # the below buffer is used for summation of the gaussians (equation from theory)\n",
    "        buffer = buffer + (1 / (math.sqrt(2*math.pi)*h_non_diabetes)) * np.exp((-1 / (2*(h_non_diabetes**2)))*(features_array_non_diabetes[:,i] - xi)**2)\n",
    "    buffer = buffer / N_non_diabetes\n",
    "    marginal_pdfs_non_diab.append(buffer)\n",
    "\n",
    "pdf_non_diabetes_parzen = np.ones(marginal_pdfs_non_diab[0].shape) # initialization for pdf calculation\n",
    "for marg_pdf in marginal_pdfs_non_diab:\n",
    "    pdf_non_diabetes_parzen = pdf_non_diabetes_parzen * marg_pdf # calculate product repeatedly\n",
    "\n",
    "print('pdf estimation for non diabetes class (naive bayes using parzen windows)\\n')\n",
    "print(pdf_non_diabetes_parzen)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEMnTWTmX5E0"
   },
   "source": [
    "<u>**Goodness of fit measument**</u>\n",
    "\n",
    "In order to calculate **AIC and BIC criterion** values for each class, we calculate firstly log-likelihood L(θ). For the assumption used for pdfs *k* (the number of estimated parameters for our model) is 1 => <u>the only parameter we can affect is *h* which is the same for every $X_{i}$</u>. Finally, regarding AIC the **general equation** is used, since N/k is more than 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMXC2-9-X5E1",
    "outputId": "cffc4070-8908-46ee-a64b-35119069ca0b"
   },
   "outputs": [],
   "source": [
    "## k = 1 because the only parameter is h which is the same for every Xi\n",
    "k_parzen = 1\n",
    "N_diabetes = len(features_array_diabetes[:,0])\n",
    "N_non_diabetes = len(features_array_non_diabetes[:,0])\n",
    "\n",
    "diabetes_loglikelihood_parzen = np.log(pdf_diabetes_parzen).sum()\n",
    "AIC_parzen_diabetes = -2*diabetes_loglikelihood_parzen + 2 * k_parzen\n",
    "BIC_parzen_diabetes = -2*diabetes_loglikelihood_parzen + k_parzen * math.log(N_diabetes)\n",
    "print('AIC_parzen_diabetes: ', AIC_parzen_diabetes)\n",
    "print('BIC_parzen_diabetes: ', BIC_parzen_diabetes)\n",
    "\n",
    "non_diabetes_loglikelihood_parzen = np.log(pdf_non_diabetes_parzen).sum()\n",
    "AIC_parzen_non_diabetes = -2*non_diabetes_loglikelihood_parzen + 2 * k_parzen \n",
    "BIC_parzen_non_diabetes = -2*non_diabetes_loglikelihood_parzen + k_parzen * math.log(N_non_diabetes)\n",
    "print('AIC_parzen_non_diabetes: ', AIC_parzen_non_diabetes)\n",
    "print('BIC_parzen_non_diabetes: ', BIC_parzen_non_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Conclusions regarding the above assumptions based on AIC and BIC criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we observe that regardless the assumption we use for the pdfs **the AIC and BIC criteria values don't differ for each class significantly**. That is because **the log-likelihood term dominates the result** and this term is <u>the same</u> for both criteria.\n",
    "\n",
    "The fact that for each assumption the AIC/BIC criteria is almost double for the non diabetes class is justified from the fact that in our dataset the samples belonging to is almost double from diabetes class samples.\n",
    "\n",
    "As for the comparison between the assumptions, we see that the assumptions for **2.2.b and 2.2.c have the smallest values with 2.2.b being slighly smaller** and so being <u>candidates for better fit</u> of the actual pdfs. **Then comes 2.2.a and the worst candidate (always according to the criteria) is the 2.2.d assumption.**\n",
    "\n",
    "The above are justified if we take into account that <u>2.2.b assumption is the more complex one and takes into account the possible dependencies between the class features</u>. The 2.2.c assumption however is <u>less complex (since it regards the class features as mutually statistically independent) but still stands as a good candidate</u> for fitting the pdfs because it takes into account the mean and variance of each feature and possibly they features actually follow gaussian distribution. <u>On the other hand the approach with naive bayes and the parzen windows doesn't seem to approach the data well. Finally, the 2.2.a assumption seems to be a simple yet moderately effective fit as it produces much smaller values for AIC and BIC than 2.2.d and not much larger than the other two approaches.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PR7oOqDG7gxR"
   },
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kUbHxax7i3o"
   },
   "outputs": [],
   "source": [
    "#  Problem 2 - iii - Bayesian classification utilizing the estimated pdfs from 2ii\n",
    "\n",
    "# importing the required modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "df = pd.read_csv('pima-indians-diabetes.data', delimiter=',', encoding='ISO-8859–1', names=['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXnI1wT2X5E2"
   },
   "source": [
    "#### 2.3.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption:** \"*Pdfs are gaussian, with diagonal covariance matrices. Mean and covariance of the pdfs are estimated using Maximum Likelihood from the available data.*\"\n",
    "\n",
    "Below we define helper functions we are going to use to implement the Bayes Classifier for the above assumption for class pdfs. *We are explaining only any new notions added to the code from question 2.ii since in general it's the same, just grouped for reusability reasons.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# this function calculates the mean for a given dataframe\n",
    "# class_feature is the key corresponding to the class\n",
    "# class_value is the value corresponding to the class whose mean we are calculating\n",
    "def calculate_mean_for_class(dataframe, class_feature, class_value):\n",
    "    mean = np.array([])\n",
    "    features = dataframe.keys()\n",
    "    features = features.drop(class_feature)\n",
    "    for key in features:\n",
    "        mean = np.append(mean, [dataframe[key][dataframe[class_feature] == class_value].sum()/len(dataframe[key][dataframe[class_feature] == class_value])])\n",
    "    return mean\n",
    "\n",
    "# this function calculates the covariance matrix for a given dataframe\n",
    "# class_mean is the mean for the class\n",
    "# class_feature is the key corresponding to the class\n",
    "# class_value is the value corresponding to the class whose mean we are calculating\n",
    "def calculate_covariance_matrix_for_class(dataframe, class_mean, class_feature, class_value):\n",
    "    features = dataframe.keys()\n",
    "    features = features.drop(class_feature)\n",
    "    dimension = len(features)\n",
    "    covariance = np.zeros(shape=(dimension, dimension))\n",
    "         \n",
    "    N_diabetes = len(dataframe[features[1]][dataframe[class_feature] == class_value])\n",
    "    buffer = 0\n",
    "    for i in range(0, dimension):\n",
    "        buffer = buffer + ((dataframe[features[i]][dataframe[class_feature] == class_value] - class_mean[i]).T).dot(dataframe[features[i]][dataframe[class_feature] == class_value] - class_mean[i])\n",
    "\n",
    "    sigma = buffer/(dimension*N_diabetes)\n",
    "\n",
    "    for i in range(0, dimension):\n",
    "        covariance[i][i] = sigma\n",
    "\n",
    "    return covariance\n",
    "\n",
    "# this function calculates pdf for a given test set x\n",
    "# mean is the mean of the train data\n",
    "# covariance is the covariance matrix of the train data\n",
    "def calculate_pdf_for_test_set(x, mean, covariance):\n",
    "    deter_covariance = np.linalg.det(covariance)\n",
    "    covariance_inv = np.linalg.inv(covariance)\n",
    "    dimension = len(mean)\n",
    "    pdf_x = (1 / ( (2*math.pi)**(dimension/2) * (deter_covariance**(1/2)))) * np.exp((-1/2)*((x.to_numpy()-mean).dot(covariance_inv)).dot((x.to_numpy()-mean).T))\n",
    "    return pdf_x.diagonal()\n",
    "\n",
    "# this function does the classification of the points in test set and calculates the accuracy\n",
    "# prop_class0, prop_class1 are the probabilities for each sample in test set to be in class0 or class1 respectively\n",
    "# test_set is the test series with the features and the actual class for each feature\n",
    "# epoch is the repetition we are in the cross validation cycle\n",
    "def classify_and_calculate_epoch_accuracy(prop_class0, prop_class1, test_set, epoch):  \n",
    "    classification = []\n",
    "    for i in range(0, len(prop_class0)):\n",
    "        # classification of each sample according to which probability is greater\n",
    "        if prop_class0[i] > prop_class1[i]:\n",
    "            classification.append(0)\n",
    "        else:\n",
    "            classification.append(1)\n",
    "    # in order to count the accuracy we count the correct classifications\n",
    "    # by comparing with actual class of each sample and finally divide with the test set size\n",
    "    correct_classifications = 0\n",
    "    for i in range(0, len(classification)):\n",
    "        if classification[i] == test_set['Y'][epoch*len(classification)+i]:\n",
    "            correct_classifications += 1\n",
    "        accurracy_percentage = correct_classifications * 100 / len(classification)\n",
    "    return accurracy_percentage\n",
    "\n",
    "# this function runs all the necessary steps to calculate the accuracy for a validation epoch\n",
    "# train_set is our training set for this epoch\n",
    "# test_set is our test set fot this epoch\n",
    "# prop_non_diabetes, prop_diabetes are the propabilities for each class for the given dataset in total\n",
    "# epoch is the repetition step in cross validation\n",
    "def calculate_accuracy_for_validation_epoch(train_set, test_set, prop_non_diabetes, prop_diabetes, epoch):\n",
    "    # calculate mean for non diabetes class\n",
    "    mean_class_non_diabetes = calculate_mean_for_class(train_set, 'Y', 0)\n",
    "    # calculate covariance for non diabetes class\n",
    "    covariance_class_non_diabetes = calculate_covariance_matrix_for_class(train_set, mean_class_non_diabetes, 'Y', 0)\n",
    "    # calculate mean for diabetes class\n",
    "    mean_class_diabetes = calculate_mean_for_class(train_set, 'Y', 1)\n",
    "    # calculate covariance for diabetes class\n",
    "    covariance_class_diabetes = calculate_covariance_matrix_for_class(train_set, mean_class_diabetes, 'Y', 1)\n",
    "    # calculate pdf for test set for non diabetes class\n",
    "    pdfs_test_set_class_non_diabetes = calculate_pdf_for_test_set(test_set.drop('Y', axis=1), mean_class_non_diabetes, covariance_class_non_diabetes)\n",
    "    prop_class_non_diabetes_test_set = pdfs_test_set_class_non_diabetes*prop_non_diabetes\n",
    "    # calculate pdf for test set for diabetes class\n",
    "    pdfs_test_set_class_diabetes = calculate_pdf_for_test_set(test_set.drop('Y', axis=1), mean_class_diabetes, covariance_class_diabetes)\n",
    "    prop_class_diabetes_test_set = pdfs_test_set_class_diabetes*prop_diabetes\n",
    "    # do the classification, the cross validation and return the accuracy for the current epoch\n",
    "    return classify_and_calculate_epoch_accuracy(prop_class_non_diabetes_test_set, prop_class_diabetes_test_set, test_set, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are utilizing the above defined functions to **perform 8-fold cross validation** for our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pima-indians-diabetes.data',delimiter=',', encoding='ISO-8859–1', names=['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'Y'])\n",
    "k = math.ceil(df.index.shape[0] / 8) # 8-Fold cross validation\n",
    "# the probabibly of each class in our data set is by definition the number of samples belonging in the class\n",
    "# devided by the size of the data set\n",
    "class_non_diabetes_prop = df[df['Y'] == 0].shape[0] / df.index.shape[0]\n",
    "class_diabete_prop = df[df['Y'] == 1].shape[0] / df.index.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Pdfs are gaussian, with diagonal covariance matrices\n",
    "accurracy_percentages = []\n",
    "for i in range(0, 8):\n",
    "    accurracy_percentages.append(calculate_accuracy_for_validation_epoch(df.drop(df.index[i*k:(i+1)*k]), df[i*k:(i+1)*k], class_non_diabetes_prop, class_diabete_prop, i))\n",
    "\n",
    "print(accurracy_percentages)\n",
    "average_accuracy = sum(accurracy_percentages) / len(accurracy_percentages)\n",
    "\n",
    "print('Average accuracy for current pdf assumption:\\n')\n",
    "print(round(average_accuracy, 2), '%')\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9QxsQd2X5E3"
   },
   "source": [
    "#### 2.3.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sgn58UHXX5E3"
   },
   "source": [
    "**Assumption:** \"*Pdfs are gaussian, with non-diagonal covariance matrices. Means and covariance matrices of the pdfs are estimated using Maximum Likelihood from the available data.*\"\n",
    "\n",
    "Below we define helper functions we are going to use to implement the Bayes Classifier for the above assumption for class pdfs. *We are explaining only any new notions added to the code from question 2.ii since in general it's the same, just grouped for reusability reasons.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crXTUDR5X5E4"
   },
   "outputs": [],
   "source": [
    "# this function calculates the mean for a given dataframe\n",
    "# class_feature is the key corresponding to the class\n",
    "# class_value is the value corresponding to the class whose mean we are calculating\n",
    "def calculate_mean_for_class(dataframe, class_feature, class_value):\n",
    "    mean = np.array([])\n",
    "    features = dataframe.keys()\n",
    "    features = features.drop(class_feature)\n",
    "    for key in features:\n",
    "        mean = np.append(mean, [dataframe[key][df[class_feature] == class_value].sum()/len(df[key][df[class_feature] == class_value])])\n",
    "    return mean\n",
    "\n",
    "# this function calculates the covariance matrix for a given dataframe\n",
    "# class_mean is the means matrix for the class\n",
    "# class_feature is the key corresponding to the class\n",
    "# class_value is the value corresponding to the class whose mean we are calculating\n",
    "def calculate_covariance_matrix_for_class(dataframe, class_mean, class_feature, class_value):\n",
    "    features = dataframe.keys()\n",
    "    features = features.drop(class_feature)\n",
    "    dimension = len(features)\n",
    "    covariance = np.zeros(shape=(dimension, dimension))\n",
    "    for i in range(0, dimension):\n",
    "        for j in range(0, dimension):\n",
    "            buffer = (df[features[i]][df[class_feature] == class_value] - class_mean[i])*(df[features[j]][df[class_feature] == class_value] - class_mean[j])\n",
    "            covariance[i][j] = buffer.sum()/len(buffer)\n",
    "\n",
    "    return covariance\n",
    "\n",
    "# this function calculates pdf for a given test set x\n",
    "# mean is the means matrix of the train data\n",
    "# covariance is the covariance matrix of the train data\n",
    "def calculate_pdf_for_test_set(x, mean, covariance):\n",
    "    deter_covariance = np.linalg.det(covariance)\n",
    "    covariance_inv = np.linalg.inv(covariance)\n",
    "    dimension = len(mean)\n",
    "    pdf_x = (1 / ( (2*math.pi)**(dimension/2) * (deter_covariance**(1/2)))) * np.exp((-1/2)*((x.to_numpy()-mean).dot(covariance_inv)).dot((x.to_numpy()-mean).T))\n",
    "    return pdf_x.diagonal()\n",
    "\n",
    "# this function does the classification of the points in test set and calculates the accuracy\n",
    "# prop_class0, prop_class1 are the probabilities for each sample in test set to be in class0 or class1 respectively\n",
    "# test_set is the test series with the features and the actual class for each feature\n",
    "# epoch is the repetition we are in the cross validation cycle\n",
    "def classify_and_calculate_epoch_accuracy(prop_class0, prop_class1, test_set, epoch):\n",
    "    classification = []\n",
    "    for i in range(0, len(prop_class0)):\n",
    "        # classification of each sample according to which probability is greater\n",
    "        if prop_class0[i] > prop_class1[i]:\n",
    "            classification.append(0)\n",
    "        else:\n",
    "            classification.append(1)\n",
    "    # in order to count the accuracy we count the correct classifications\n",
    "    # by comparing with actual class of each sample and finally divide with the test set size\n",
    "    correct_classifications = 0\n",
    "    for i in range(0, len(classification)):\n",
    "        if classification[i] == test_set['Y'][epoch*len(classification)+i]:\n",
    "            correct_classifications += 1\n",
    "    accurracy_percentage = correct_classifications * 100 / len(classification) # we return % percentage\n",
    "    return accurracy_percentage\n",
    "\n",
    "# this function runs all the necessary steps to calculate the accuracy for a validation epoch\n",
    "# train_set is our training set for this epoch\n",
    "# test_set is our test set fot this epoch\n",
    "# prop_non_diabetes, prop_diabetes are the propabilities for each class for the given dataset in total\n",
    "# epoch is the repetition step in cross validation\n",
    "def calculate_accuracy_for_validation_epoch(train_set, test_set, prop_non_diabetes, prop_diabetes, epoch):\n",
    "    # calculate mean for non diabetes class\n",
    "    mean_class_non_diabetes = calculate_mean_for_class(train_set, 'Y', 0)\n",
    "    # calculate cov matrix for non diabetes class\n",
    "    covariance_class_non_diabetes = calculate_covariance_matrix_for_class(train_set, mean_class_non_diabetes, 'Y', 0)\n",
    "    # calculate mean for diabetes class\n",
    "    mean_class_diabetes = calculate_mean_for_class(train_set, 'Y', 1)\n",
    "    # calculate cov matrix for non diabetes class\n",
    "    covariance_class_diabetes = calculate_covariance_matrix_for_class(train_set, mean_class_diabetes, 'Y', 1)\n",
    "    # calculate pdf for test set for non diabetes class\n",
    "    pdfs_test_set_class_non_diabetes = calculate_pdf_for_test_set(test_set.drop('Y', axis=1), mean_class_non_diabetes, covariance_class_non_diabetes)\n",
    "    # calculate probabilities for each sample in test set to be in non diabetes class\n",
    "    prop_class_non_diabetes_test_set = pdfs_test_set_class_non_diabetes*prop_non_diabetes\n",
    "    # calculate pdf for test set for diabetes class\n",
    "    pdfs_test_set_class_diabetes = calculate_pdf_for_test_set(test_set.drop('Y', axis=1), mean_class_diabetes, covariance_class_diabetes)\n",
    "    # calculate probabilities for each sample in test set to be in diabetes class\n",
    "    prop_class_diabetes_test_set = pdfs_test_set_class_diabetes*prop_diabetes\n",
    "    # do the classification, the cross validation and return the accuracy for the current epoch\n",
    "    return classify_and_calculate_epoch_accuracy(prop_class_non_diabetes_test_set, prop_class_diabetes_test_set, test_set, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yih26QEIX5E4"
   },
   "source": [
    "Below we are utilizing the above defined functions to **perform 8-fold cross validation** for our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OrSpS-yX5E5",
    "outputId": "dfc87f4e-37eb-41cf-958b-085f85ff3d76"
   },
   "outputs": [],
   "source": [
    "fold = 8\n",
    "k = math.ceil(df.index.shape[0] / fold) # 8-Fold cross validation\n",
    "\n",
    "# the probabibly of each class in our data set is by definition the number of samples belonging in the class\n",
    "# devided by the size of the data set\n",
    "class_non_diabetes_prop = df[df['Y'] == 0].shape[0] / df.index.shape[0]\n",
    "class_diabete_prop = df[df['Y'] == 1].shape[0] / df.index.shape[0]\n",
    "\n",
    "accurracy_percentages = []\n",
    "for i in range(0, fold):\n",
    "  accurracy_percentages.append(calculate_accuracy_for_validation_epoch(df.drop(df.index[i*k:(i+1)*k]), df[i*k:(i+1)*k], class_non_diabetes_prop, class_diabete_prop, i))\n",
    "\n",
    "print('Accuracy percentages for each \"epoch\":\\n')\n",
    "print(accurracy_percentages)\n",
    "print('\\n')\n",
    "\n",
    "average_accuracy = sum(accurracy_percentages) / len(accurracy_percentages)\n",
    "\n",
    "print('Average accuracy for current pdf assumption:\\n')\n",
    "print(round(average_accuracy, 2), '%')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-KQmLS6X5E8"
   },
   "source": [
    "#### 2.3.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption:** \"*Components of the feature vectors are mutually statistically independent (the usual naïve Bayes approach). Marginal Pdfs are gaussian, with parameters (mean, variance) estimated using Maximum Likelihood from the available data.*\"\n",
    "\n",
    "Below we define helper functions we are going to use to implement the Bayes Classifier for the above assumption for class pdfs. *We are explaining only any new notions added to the code from question 2.ii since in general it's the same, just grouped for reusability reasons.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates the mean for a given dataframe and for every feature\n",
    "# class_feature is the key corresponding to the class\n",
    "# class_value is the value corresponding to the class whose mean we are calculating\n",
    "#feature_value is the value for the current feature\n",
    "def calculate_mean_per_feature_for_class(dataframe, class_feature, class_value, feature_value):\n",
    "    N_diabetes_feature = len(dataframe[feature_value][dataframe[class_feature] == class_value])\n",
    "    features_diabetes = dataframe[feature_value][dataframe[class_feature] == class_value]\n",
    "    mean =  dataframe[feature_value][dataframe[class_feature] == class_value].sum()/len(dataframe[feature_value][dataframe[class_feature] == class_value])\n",
    "    return mean\n",
    "                      \n",
    "# this function calculates the variance for a given dataframe and for every feature\n",
    "# class_feature is the key corresponding to the class\n",
    "# class_value is the value corresponding to the class whose mean we are calculating\n",
    "#feature_value is the value for the current feature\n",
    "def calculate_variance_per_feature_for_class(dataframe,mean, class_feature, class_value, feature_value):\n",
    "    N_diabetes_feature = len(dataframe[feature_value][dataframe[class_feature] == class_value])\n",
    "    features_diabetes = dataframe[feature_value][dataframe[class_feature] == class_value]  \n",
    "    buffer_marg = 0\n",
    "                                               \n",
    "    for xi in features_diabetes:\n",
    "        buffer_marg = buffer_marg + (xi - mean)*(xi - mean)\n",
    "      \n",
    "    variance = buffer_marg/(N_diabetes_feature) \n",
    "    return variance\n",
    "\n",
    "# this function calculates pdf for the marginal pdfs for a given test set x\n",
    "# mean is the mean of the train data\n",
    "# covariance is the covariance matrix of the train data\n",
    "def calculate_pdf_marginal_for_test_set(x,mean, variance,class_feature,class_value,marginal_pdfs,feature_value):                                               \n",
    "    buffer_array = x[feature_value]-mean\n",
    "    buffer = (1 / ( (2*math.pi)**(1/2) * (variance**(1/2)))) * np.exp((-1/2)*(buffer_array**2)/(variance))\n",
    "    marginal_pdfs.append(buffer)\n",
    "    \n",
    "    return marginal_pdfs\n",
    "\n",
    "# this function calculates the total pdf according to the marginal pdfs which are calculated\n",
    "# marginal_pdfs the marginal gaussian pdfs\n",
    "def calculate_total_pdf(marginal_pdfs):\n",
    "    pdf_gaussian = np.ones(marginal_pdfs[0].shape)\n",
    "    for marg_pdf in marginal_pdfs:\n",
    "        pdf_gaussian = pdf_gaussian * marg_pdf\n",
    "    return pdf_gaussian\n",
    "\n",
    "# this function does the classification of the points in test set and calculates the accuracy\n",
    "# prop_class0, prop_class1 are the probabilities for each sample in test set to be in class0 or class1 respectively\n",
    "# test_set is the test series with the features and the actual class for each feature\n",
    "# epoch is the repetition we are in the cross validation cycle\n",
    "def classify_and_calculate_epoch_accuracy_with_marginal(prop_class0, prop_class1, test_set, epoch):\n",
    "    classification = []\n",
    "    for i in range(0, len(prop_class0)):\n",
    "        if prop_class0[i] > prop_class1[i]:\n",
    "            classification.append(0)\n",
    "        else:\n",
    "            classification.append(1)\n",
    "    correct_classifications = 0\n",
    "    for i in range(0, len(classification)):\n",
    "        if classification[i] == test_set['Y'][epoch*len(classification)+i]:\n",
    "            correct_classifications += 1\n",
    "    accurracy_percentage = correct_classifications * 100 / len(classification)\n",
    "    return accurracy_percentage\n",
    "\n",
    "# this function runs all the necessary steps to calculate the accuracy for a validation epoch\n",
    "# In this case firstly we calculate the mean and the variance for each feature in order to calculate the marginal gaussian pdfs\n",
    "# and then we calculate the total pdf.\n",
    "# train_set is our training set for this epoch\n",
    "# test_set is our test set fot this epoch\n",
    "# prop_non_diabetes, prop_diabetes are the propabilities for each class for the given dataset in total\n",
    "# epoch is the repetition step in cross validation\n",
    "def calculate_accuracy_marginal_for_validation_epoch(train_set, test_set, prop_non_diabetes, prop_diabetes, epoch):  \n",
    "    features = train_set.keys()\n",
    "    features = features.drop('Y')\n",
    "    dimension = len(features)\n",
    "    marginal_pdfs_test_set_class_non_diabetes = []\n",
    "    marginal_pdfs_test_set_class_diabetes = []\n",
    "    for i in range(0,dimension):\n",
    "        # calculate mean for non diabetes class\n",
    "        mean_class_per_feature_non_diabetes = calculate_mean_per_feature_for_class(train_set, 'Y', 0,features[i])\n",
    "        # calculate variance for non diabetes class\n",
    "        variance_class_per_feature_non_diabetes = calculate_variance_per_feature_for_class(train_set, mean_class_per_feature_non_diabetes, 'Y', 0,features[i])\n",
    "        # calculate mean for diabetes class\n",
    "        mean_class_per_feature_diabetes = calculate_mean_per_feature_for_class(train_set, 'Y', 1,features[i])\n",
    "        # calculate variance for diabetes class\n",
    "        variance_class_per_feature_diabetes = calculate_variance_per_feature_for_class(train_set, mean_class_per_feature_diabetes, 'Y', 1,features[i])\n",
    "        # calculate the marginal pdfs for non diabetes class\n",
    "        marginal_pdfs_test_set_class_non_diabetes = calculate_pdf_marginal_for_test_set(test_set, mean_class_per_feature_non_diabetes, variance_class_per_feature_non_diabetes,'Y',1,marginal_pdfs_test_set_class_non_diabetes,features[i])        \n",
    "        # calculate the marginal pdfs for diabetes class\n",
    "        marginal_pdfs_test_set_class_diabetes = calculate_pdf_marginal_for_test_set(test_set, mean_class_per_feature_non_diabetes, variance_class_per_feature_non_diabetes,'Y',1,marginal_pdfs_test_set_class_diabetes,features[i])\n",
    "    # calculate the total pdf for non diabetes class\n",
    "    total_gaussian_pdf_non_diabetes = calculate_total_pdf(marginal_pdfs_test_set_class_non_diabetes)    \n",
    "    # calculate the total pdf for diabetes class\n",
    "    total_gaussian_pdf_diabetes = calculate_total_pdf(marginal_pdfs_test_set_class_diabetes)\n",
    "    prop_class_non_diabetes_test_set = total_gaussian_pdf_non_diabetes.to_numpy()*prop_non_diabetes\n",
    "    prop_class_diabetes_test_set = total_gaussian_pdf_diabetes.to_numpy()*prop_diabetes\n",
    "    # do the classification, the cross validation and return the accuracy for the current epoch\n",
    "    return classify_and_calculate_epoch_accuracy_with_marginal(prop_class_non_diabetes_test_set, prop_class_diabetes_test_set, test_set, epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are utilizing the above defined functions to **perform 8-fold cross validation** for our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pima-indians-diabetes.data',delimiter=',', encoding='ISO-8859–1', names=['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'Y'])\n",
    "k = math.ceil(df.index.shape[0] / 8) # 8-Fold cross validation\n",
    "\n",
    "# the probabibly of each class in our data set is by definition the number of samples belonging in the class\n",
    "# devided by the size of the data set\n",
    "class_non_diabetes_prop = df[df['Y'] == 0].shape[0] / df.index.shape[0]\n",
    "class_diabete_prop = df[df['Y'] == 1].shape[0] / df.index.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Marginal Pdfs are gaussian,ML method\n",
    "accurracy_percentages_with_marginals = []\n",
    "for i in range(0, 8):\n",
    "    accurracy_percentages_with_marginals.append(calculate_accuracy_marginal_for_validation_epoch(df.drop(df.index[i*k:(i+1)*k]), df[i*k:(i+1)*k], class_non_diabetes_prop, class_diabete_prop, i))\n",
    "\n",
    "print(accurracy_percentages_with_marginals)\n",
    "average_accuracy = sum(accurracy_percentages_with_marginals) / len(accurracy_percentages_with_marginals)\n",
    "\n",
    "print('Average accuracy for current pdf assumption:\\n')\n",
    "print(round(average_accuracy, 2), '%')\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhzRGBNbX5E9"
   },
   "source": [
    "#### 2.3.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8S0y4z2X5E_"
   },
   "source": [
    "**Assumption:** \"*Components of the feature vectors are mutually statistically independent (the usual naïve Bayes approach). Marginal pdfs are computed using 1-d Parzen windows with gaussian kernels. Take the width h of each window equal to the square root of the number of patterns in the available data.*\"\n",
    "\n",
    "Below we define helper functions we are going to use to implement the Bayes Classifier for the above assumption for class pdfs. *We are explaining only any new notions added to the code from question 2.ii since in general it's the same, just grouped for reusability reasons.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dKjOdxxX5FA"
   },
   "outputs": [],
   "source": [
    "# this function calculates the pdf for the test set using 1-d parzen windows with gaussian kernels for the marginal pdfs\n",
    "# it receives 2 arguments: the training and the test set in the form of pandas series\n",
    "def calculate_pdf_parzen_for_test_set(x_train, x_test):\n",
    "    x_train_array = x_train.to_numpy() # turn train series into array\n",
    "    x_test_array = x_test.to_numpy() # turn test series into array\n",
    "    N = x_train_array.shape[0] # size of training set\n",
    "    h = math.sqrt(N) # set the window width equal to the square root of number of samples for class\n",
    "    dimension = x_train_array.shape[1] # problem dimensionality\n",
    "    \n",
    "    # marginal pdfs calculation\n",
    "    marginal_pdfs = []\n",
    "    for i in range(0, dimension):\n",
    "        buffer = np.zeros(x_test_array.shape[0])\n",
    "        for xi in x_train_array[:][i]:\n",
    "            buffer = buffer + (1 / (math.sqrt(2*math.pi)*h)) * np.exp((-1 / (2*(h**2)))*(x_test_array[:,i] - xi)**2)\n",
    "        buffer = buffer / N\n",
    "        marginal_pdfs.append(buffer)\n",
    "    \n",
    "    # pdf calculation as product of the marginal pdfs according to naive bayes approach\n",
    "    pdf_parzen = np.ones(marginal_pdfs[0].shape)\n",
    "    for marg_pdf in marginal_pdfs:\n",
    "        pdf_parzen = pdf_parzen * marg_pdf\n",
    "    return pdf_parzen\n",
    "\n",
    "# this function does the classification of the points in test set and calculates the accuracy\n",
    "# prop_class0, prop_class1 are the probabilities for each sample in test set to be in class0 or class1 respectively\n",
    "# test_set is the test series with the features and the actual class for each feature\n",
    "# epoch is the repetition we are in the cross validation cycle\n",
    "def classify_and_calculate_epoch_accuracy(prop_class0, prop_class1, test_set, epoch):\n",
    "    classification = []\n",
    "    for i in range(0, len(prop_class0)):\n",
    "        # classification of each sample according to which probability is greater\n",
    "        if prop_class0[i] > prop_class1[i]:\n",
    "            classification.append(0)\n",
    "        else:\n",
    "            classification.append(1)\n",
    "    # in order to count the accuracy we count the correct classifications\n",
    "    # by comparing with actual class of each sample and finally divide with the test set si\n",
    "    correct_classifications = 0\n",
    "    for i in range(0, len(classification)):\n",
    "        if classification[i] == test_set['Y'][epoch*len(classification)+i]:\n",
    "            correct_classifications += 1\n",
    "    accurracy_percentage = correct_classifications * 100 / len(classification) # we return % percentage\n",
    "    return accurracy_percentage\n",
    "\n",
    "# this function runs all the necessary steps to calculate the accuracy for a validation epoch\n",
    "# train_set is our training set for this epoch\n",
    "# test_set is our test set fot this epoch\n",
    "# prop_non_diabetes, prop_diabetes are the propabilities for each class for the given dataset in total\n",
    "# epoch is the repetition step in cross validation\n",
    "def calculate_accuracy_parzen_for_validation_epoch(train_set, test_set, prop_non_diabetes, prop_diabetes, epoch):\n",
    "    # calculate pdf for test set for non diabetes class\n",
    "    pdfs_parzen_test_set_class_non_diabetes = calculate_pdf_parzen_for_test_set(train_set[train_set['Y'] == 0].drop('Y', axis=1), test_set.drop('Y', axis=1))\n",
    "    # calculate probabilities for each sample in test set to be in non diabetes class\n",
    "    prop_class_non_diabetes_test_set = pdfs_parzen_test_set_class_non_diabetes*prop_non_diabetes\n",
    "    # calculate pdf for test set for diabetes class\n",
    "    pdfs_parzen_test_set_class_diabetes = calculate_pdf_parzen_for_test_set(train_set[train_set['Y'] == 1].drop('Y', axis=1), test_set.drop('Y', axis=1))\n",
    "    # calculate probabilities for each sample in test set to be in diabetes class\n",
    "    prop_class_diabetes_test_set = pdfs_parzen_test_set_class_diabetes*prop_diabetes\n",
    "    # do the classification, the cross validation and return the accuracy for the current epoch\n",
    "    return classify_and_calculate_epoch_accuracy(prop_class_non_diabetes_test_set, prop_class_diabetes_test_set, test_set, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hhboAlSX5FB"
   },
   "source": [
    "Below we are utilizing the above defined functions to **perform 8-fold cross validation** for our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWL8RjhXX5FC",
    "outputId": "1b881cff-d884-4749-ab1a-ea32f32ac700"
   },
   "outputs": [],
   "source": [
    "fold = 8\n",
    "k = math.ceil(df.index.shape[0] / fold) # 8-Fold cross validation\n",
    "\n",
    "# the probabibly of each class in our data set is by definition the number of samples belonging in the class\n",
    "# devided by the size of the data set\n",
    "class_non_diabetes_prop = df[df['Y'] == 0].shape[0] / df.index.shape[0]\n",
    "class_diabete_prop = df[df['Y'] == 1].shape[0] / df.index.shape[0]\n",
    "\n",
    "accurracy_percentages_parzen = []\n",
    "for i in range(0, fold):\n",
    "    accurracy_percentages_parzen.append(calculate_accuracy_parzen_for_validation_epoch(df.drop(df.index[i*k:(i+1)*k]), df[i*k:(i+1)*k], class_non_diabetes_prop, class_diabete_prop, i))\n",
    "\n",
    "print('Accuracy percentages for each \"epoch\":\\n')\n",
    "print(accurracy_percentages_parzen)\n",
    "print('\\n')\n",
    "\n",
    "average_accuracy_parzen = sum(accurracy_percentages_parzen) / len(accurracy_percentages_parzen)\n",
    "\n",
    "print('Average accuracy for current pdf assumption:\\n')\n",
    "print(round(average_accuracy_parzen, 2), '%')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "cZ_Ifv7LX5FD"
   },
   "source": [
    "#### 2.3 Conclusions and comparison with k-NN classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We **would expect** that more accurate estimates for the pdfs always tend to improve accuracy as well. **However, the results show us that this is partly true**. For example in 2.2 question we saw that the best and worst candidates for pdfs estimation where the assumptions in b and d sub-question respectively. **The average accuracy for these two Bayes classifiers proves this**.\n",
    "\n",
    "<u>**This is not the case though regarding the assumptions in a and c sub-question**</u>. First of all, the naive bayes approach for sub-question c had approximately the same AIC and BIC values with b sub-question. **However, it's average accuracy is about 10% worst than the approach with the non-diagonal covariance matrices for the pdfs, which shows us that probably the features/components are not mutually statistically independent**. Moreover, <u>the accuracy for sub-question c pdf assuption is slightly worse than the approach with the diagonal covariance matrices of sub-question a</u>, although according to AIC and BIC criteria the later is a less accurate estimate of the pdf.\n",
    "\n",
    "***From all the above we conclude that the AIC/BIC criteria are a good first measure of aborting some estimates or tend in favour of some others, but not always say the truth about the actual accuracy of the classifier.***\n",
    "\n",
    "**In comparison with the k-NN classifier from 2.1**, we observe that <u>the best Bayes classifier</u> from the assumptions conducted has about <u>5% better accuracy than the best k-NN classifier</u>. However, the <u>k-NN classifier</u> for all the range of k performs with **accuracy above 65%** whereas the rest Bayes classifiers based on an assumption different than the best one produce less accurate results, which depending on how bad an assumption about the pdfs is may be even less than 50% accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59VomALE7jRV"
   },
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ugab8zz6fgQ"
   },
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "09Dqc6VLnX2c"
   },
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAuAGVr16y2W"
   },
   "source": [
    "Functions\n",
    "> General functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LCmhIy7jLhg1"
   },
   "outputs": [],
   "source": [
    "# Read data from file\n",
    "def load_file(filename):\n",
    "    dataset = list()\n",
    "    count = 0\n",
    "    with open(filename, 'r') as file:\n",
    "        file_reader = reader(file)\n",
    "        for row in file_reader:\n",
    "            count += 1  #count rows\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return (dataset, count)\n",
    "\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEr6VF4HLpYA"
   },
   "source": [
    "\n",
    "\n",
    "> Specific functions used in main perceptron algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "j_4yZltPLirn"
   },
   "outputs": [],
   "source": [
    "def multiply_data_w_weights(input, w):\n",
    "    t_w_x = 0\n",
    "    for idx, inp in enumerate(input):\n",
    "        t_w_x += w[idx]*inp\n",
    "    return t_w_x\n",
    "\n",
    "# returns line vector for update or 0 vector for not update\n",
    "def activation_f(t_w_x, epsilon, line):\n",
    "    if t_w_x < epsilon:\n",
    "        return line           #Update\n",
    "    else:\n",
    "        return [0]*len(line)  #No Update\n",
    "\n",
    "\n",
    "def activation_decision(t_w_x, epsilon):\n",
    "    if t_w_x < epsilon:\n",
    "        return 1  #Update\n",
    "    else:\n",
    "        return 0  #No Update\n",
    "\n",
    "# Creates 2 classes noting one class as '(1)' and '(-1)' for the other 2\n",
    "# Also multiply elements by target output (x*t)\n",
    "def create_two_classes(dataset, cl_a):\n",
    "    new_a=list()\n",
    "\n",
    "    for i in dataset:\n",
    "        k=list()\n",
    "        if i[-1] == cl_a:\n",
    "            i[-1] = -1\n",
    "            new_a.append(i)\n",
    "        else:\n",
    "            # # for idx,j in enumerate(i):\n",
    "            k=[j * -1 for j in i]\n",
    "            i=k\n",
    "            i[-1] = 1\n",
    "            new_a.append(i)\n",
    "    return (new_a)\n",
    "\n",
    "\n",
    "def update_weights(dw,w_prev):\n",
    "    w_new=list()\n",
    "    for idx,el in enumerate(w_prev):\n",
    "        w_new.append(el+dw[idx])\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9sr900cflYl"
   },
   "source": [
    "Main Classification Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_4VY1qqm7jCz"
   },
   "outputs": [],
   "source": [
    "def classify_sets(w_dataset,epsilon,row_len,sel):\n",
    "\n",
    "    #Initialize variables\n",
    "    upd_vector = []\n",
    "    weights=[0.0]*row_len   #initial weight vector\n",
    "    w_prev=[1]*row_len\n",
    "    line_prev=[0]*row_len\n",
    "\n",
    "    # Parameters\n",
    "    count=0\n",
    "    epochs_number=1000\n",
    "  \n",
    "    # lines of file\n",
    "    lin_sep =[0]*(num_lines-1)  #last line of file that is counted has no data\n",
    "\n",
    "    for rep in range(epochs_number):\n",
    "\n",
    "        upd_vector=[]\n",
    "\n",
    "        for idx,line in enumerate(w_dataset):\n",
    "            dw=[]\n",
    "            w_prev=weights\n",
    "            twx = multiply_data_w_weights(line, weights)  #t*weight*data\n",
    "            upd_vector.append(activation_decision(twx, epsilon))\n",
    "            dw =activation_f(twx, epsilon, line)    \n",
    "            weights=update_weights(dw,w_prev)\n",
    "\n",
    "        if upd_vector!=lin_sep:\n",
    "            count+=1\n",
    " \n",
    "    if upd_vector==lin_sep:\n",
    "        print(\"The dataset\", str(sel), \"is linear separable with the group of the rest two.\\nThe perceptron algorithm converged in\",str(count), \"epoch(s)\\n\")\n",
    "        print(\"Hyperplane dimensions\", weights,'\\n\\n')   \n",
    "    else:\n",
    "        print(\"The dataset\", str(sel), \"is not linear separable with the group of the rest two.\\nThe perceptron algorithm did not converge after:\", str(epochs_number), \"epoch(s)\\n\\n\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHgn4h2mAjTV"
   },
   "source": [
    "Main\n",
    "\n",
    ">**Prepare** dataset element types\n",
    "\n",
    "> **Classify** one dataset against the other two classes left "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IiQMDeO3Ajzz",
    "outputId": "ccaa317c-50b8-41cb-a0de-a2070ea82deb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We note: \n",
      "1: for 'Iris-setosa', \n",
      "2: for 'Iris-versicolor', \n",
      "3: for 'Iris-virginica'\n",
      "\n",
      "The dataset 1 is linear separable with the group of the rest two.\n",
      "The perceptron algorithm converged in 4 epoch(s)\n",
      "\n",
      "Hyperplane dimensions [1.299999999999999, 5.1, -6.800000000000001, -3.0999999999999996, -1.0] \n",
      "\n",
      "\n",
      "The dataset 2 is not linear separable with the group of the rest two.\n",
      "The perceptron algorithm did not converge after: 1000 epoch(s)\n",
      "\n",
      "\n",
      "The dataset 3 is not linear separable with the group of the rest two.\n",
      "The perceptron algorithm did not converge after: 1000 epoch(s)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main \n",
    "epsilon= 1\n",
    "num_lines = 0\n",
    "filenameX = 'iris.data'\n",
    "in_dataset, num_lines = load_file(filenameX)\n",
    "\n",
    "# convert input from string to numbers\n",
    "row_len = len(in_dataset[0])\n",
    "for i in range(row_len-1):    \n",
    "    str_column_to_float(in_dataset, i)\n",
    "\n",
    "# Replace class names with numbers (like a dictionary: 1 for 'Iris-setosa', 2 for 'Iris-setosa', 3 for 'Iris-virginica')\n",
    "correlate_dict = dict()\n",
    "class_names = sorted(set([row[-1] for row in in_dataset]))\n",
    "for i, cl_name in enumerate(class_names,1):\n",
    "    correlate_dict[cl_name] = i\n",
    "for row in in_dataset:\n",
    "    row[-1] = correlate_dict[row[-1]]   #here we substitute class name \"iris-setosa with 1.. etc..\"\n",
    "\n",
    "# Results\n",
    "print(\"We note: \\n1: for 'Iris-setosa', \\n2: for 'Iris-versicolor', \\n3: for 'Iris-virginica'\\n\")\n",
    "\n",
    "for sel in range(1,4):\n",
    "    y_dataset = create_two_classes(in_dataset,int(sel))\n",
    "    classify_sets(y_dataset,epsilon,row_len,sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above we can have an insight about the *relevance* among the classes: 1, 2 & 3.\n",
    "\n",
    "Since the only possible combination that can be linear **separable** with a hyper plane is: class 1 separated by the group of [class 2 & class 3]\n",
    "(and it is NOT possible that class 2 to be separated by group [class 1 & class 3], or class 3 to be separated by group [class 1 & class 2]),we can conclude that there is a bigger \"entanglement\" or \"**relevance**\" between the data of class 2 & class 3."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9eF3ETV56zNJ",
    "A_kirO4W7AKE",
    "09d10p0R7xI8",
    "bnonK-Uf72_n",
    "RVjLC4YH78GQ",
    "XT9glXC679y8"
   ],
   "name": "Machine_LearningAssignment_1_Deliverables.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
